URL: https://www.ibm.com/think/topics/ai-vs-machine-learning-vs-deep-learning-vs-neural-networks
==================================================
AI vs. machine learning vs. deep learning vs. neural networks: What’s the difference?
These computer science terms are often used interchangeably, but what differences make each a unique technology?
Technology is becoming more embedded in our daily lives by the minute. To keep up with the pace of consumer expectations, companies are relying more heavily on machine learning algorithms to make things easier. You can see its application in social media (through object recognition in photos) or in talking directly to devices (such as Alexa or Siri).
While artificial intelligence (AI), machine learning (ML), deep learning and neural networks are related technologies, the terms are often used interchangeably, which frequently leads to confusion about their differences. This blog post clarifies some of the ambiguity.
The latest tech news, backed by expert insights
Stay up to date on the most important—and intriguing—industry trends on AI, automation, data and beyond with the Think newsletter. See the IBM Privacy Statement.
Thank you! You are subscribed.
Your subscription will be delivered in English. You will find an unsubscribe link in every newsletter. You can manage your subscriptions or unsubscribe  here. Refer to our  IBM Privacy Statement for more information.
Your subscription will be delivered in English. You will find an unsubscribe link in every newsletter. You can manage your subscriptions or unsubscribe here. Refer to our IBM Privacy Statement for more information.
How do AI, machine learning, deep learning and neural networks relate to each other?
The easiest way to think about AI, machine learning, deep learning and neural networks is to think of them as a series of AI systems from largest to smallest, each encompassing the next.
AI is the overarching system. Machine learning is a subset of AI. Deep learning is a subfield of machine learning, and neural networks make up the backbone of deep learning algorithms. It’s the number of node layers, or depth, of neural networks that distinguishes a single neural network from a deep learning algorithm, which must have more than three.
Artificial intelligence or AI, the broadest term of the three, is used to classify machines that mimic human intelligence and human cognitive functions like problem-solving and learning. AI uses predictions and automation to optimize and solve complex tasks that humans have historically done, such as facial and speech recognition, decision-making and translation.
The three main categories of AI are:
ANI is considered “weak” AI, whereas the other two types are classified as “strong” AI. We define weak AI by its ability to complete a specific task, like winning a chess game or identifying a particular individual in a series of photos. Natural language processing and computer vision, which let companies automate tasks and underpin chatbots and virtual assistants such as Siri and Alexa, are examples of ANI. Computer vision is a factor in the development of self-driving cars.
Stronger forms of AI, like AGI and ASI, incorporate human behaviors more prominently, such as the ability to interpret tone and emotion. Strong AI is defined by its ability compared to humans. AGI would perform on par with another human, while ASI, also known as superintelligence, would surpass a human’s intelligence and ability. Neither form of Strong AI exists yet, but research in this field is ongoing.
Using AI for business
An increasing number of businesses, about 35% globally, are using AI, and another 42% are exploring the technology. The development of generative AI, which uses powerful foundation models that train on large amounts of unlabeled data, can be adapted to new use cases and bring flexibility and scalability that is likely to accelerate the adoption of AI significantly. In early tests, IBM has seen generative AI bring time to value up to 70% faster than traditional AI.
Whether you use AI applications based on ML or foundation models, AI can give your business a competitive advantage. Integrating customized AI models into your workflows and systems, and automating functions such as customer service, supply chain management and cybersecurity, can help a business meet customers’ expectations, both today and as they increase in the future.
The key is identifying the right data sets from the start to help ensure that you use quality data to achieve the most substantial competitive advantage. You’ll also need to create a hybrid, AI-ready architecture that can successfully use data wherever it lives, on mainframes, data centers, in private and public clouds and at the edge.
Your AI must be trustworthy because anything less means risking damage to a company’s reputation and bringing regulatory fines. Misleading models and those containing bias or that hallucinate can come at a high cost to customers’ privacy, data rights and trust. Your AI must be explainable, fair and transparent.
What is machine learning?
Machine learning is a subset of AI that allows for optimization. When set up correctly, it helps you make predictions that minimize the errors that arise from merely guessing. For example, companies like Amazon use machine learning to recommend products to a specific customer based on what they’ve looked at and bought before.
Classic or “nondeep” machine learning depends on human intervention to allow a computer system to identify patterns, learn, perform specific tasks and provide accurate results. Human experts determine the hierarchy of features to understand the differences between data inputs, usually requiring more structured data to learn.
For example, let’s say I showed you a series of images of different types of fast food: “pizza,” “burger” and “taco.” A human expert working on those images would determine the characteristics distinguishing each picture as a specific fast food type. The bread in each food type might be a distinguishing feature. Alternatively, they might use labels, such as “pizza,” “burger” or “taco” to streamline the learning process through supervised learning.
While the subset of AI called deep machine learning can leverage labeled data sets to inform its algorithm in supervised learning, it doesn’t necessarily require a labeled data set. It can ingest unstructured data in its raw form (for example, text, images), and it can automatically determine the set of features that distinguish “pizza,” “burger” and “taco” from one another. As we generate more big data, data scientists use more machine learning. For a deeper dive into the differences between these approaches, check out Supervised versus unsupervised learning: What’s the difference?
A third category of machine learning is reinforcement learning, where a computer learns by interacting with its surroundings and getting feedback (rewards or penalties) for its actions. And online learning is a type of ML where a data scientist updates the ML model as new data becomes available.
To learn more about machine learning, check out the following video:
How deep learning differs from machine learning
As our article on deep learning explains, deep learning is a subset of machine learning. The primary difference between machine learning and deep learning is how each algorithm learns and how much data each type of algorithm uses.
Deep learning automates much of the feature extraction piece of the process, eliminating some of the manual human intervention required. It also enables the use of large data sets, earning the title of scalable machine learning. That capability is exciting as we explore the use of unstructured data further, particularly since over 80% of an organization’s data is estimated to be unstructured.
Observing patterns in the data allows a deep-learning model to cluster inputs appropriately. Taking the same example from earlier, we might group pictures of pizzas, burgers and tacos into their respective categories based on the similarities or differences identified in the images. A deep-learning model requires more data points to improve accuracy, whereas a machine-learning model relies on less data given its underlying data structure. Enterprises generally use deep learning for more complex tasks, like virtual assistants or fraud detection.
Decoding AI: Weekly News Roundup
Join our world-class panel of engineers, researchers, product leaders and more as they cut through the AI noise to bring you the latest in AI news and insights.
What is a neural network?
Neural networks, also called artificial neural networks or simulated neural networks, are a subset of machine learning and are the backbone of deep learning algorithms. They are called “neural” because they mimic how neurons in the brain signal one another.
Neural networks are made up of node layers, an input layer, one or more hidden layers and an output layer. Each node is an artificial neuron that connects to the next, and each has a weight and threshold value. When one node’s output is above the threshold value, that node is activated and sends its data to the network’s next layer. If it’s below the threshold, no data passes along.
Training data teach neural networks and help improve their accuracy over time. Once the learning algorithms are fined-tuned, they become powerful computer science and AI tools because they allow us to quickly classify and cluster data. Using neural networks, speech and image recognition tasks can happen in minutes instead of the hours they take when done manually. Google’s search algorithm is a well-known example of a neural network.
What’s the difference between deep learning and neural networks?
As mentioned in the explanation of neural networks above, but worth noting more explicitly, the “deep” in deep learning refers to the depth of layers in a neural network. A neural network of more than three layers, including the inputs and the output, can be considered a deep-learning algorithm. That can be represented by the following diagram:
Most deep neural networks are feed-forward, meaning they only flow in one direction from input to output. However, you can also train your model through backpropagation, meaning moving in the opposite direction, from output to input. Backpropagation allows us to calculate and attribute the error that is associated with each neuron, allowing us to adjust and fit the algorithm appropriately.
Managing your AI data
While all these areas of AI can help streamline areas of your business and improve your customer experience, achieving AI goals can be challenging because you’ll first need to ensure that you have the right systems to construct learning algorithms to manage your data. Data management is more than merely building the models that you use for your business. You need a place to store your data and mechanisms for cleaning it and controlling for bias before you can start building anything.
IBM, machine learning and AI
At IBM we are combining the power of machine learning and artificial intelligence in our new studio for foundation models, generative AI and machine learning, IBM® watsonx.ai™.
Subscribe to the Think Newsletter
Learn how to choose the right approach in preparing datasets and employing foundation models.
Learn how to choose the right approach in preparing datasets and employing foundation models.
Learn why IBM has been recognized as a Leader in the 2025 Gartner® Magic Quadrant™ for Data Science and Machine Learning Platforms.
Learn why IBM has been recognized as a Leader in the 2025 Gartner® Magic Quadrant™ for Data Science and Machine Learning Platforms.
Learn how organizations are shifting from launching AI in disparate pilots to using it to drive transformation at the core.
Learn how organizations are shifting from launching AI in disparate pilots to using it to drive transformation at the core.
Access our full catalog of over 100 online courses by purchasing an individual or multi-user subscription today, enabling you to expand your skills across a range of our products at a low price.
Access our full catalog of over 100 online courses by purchasing an individual or multi-user subscription today, enabling you to expand your skills across a range of our products at a low price.
IBM® Granite® is a family of open, performant and trusted AI models tailored for business and optimized to scale your AI applications. Explore language, code, time series and guardrail options.
IBM® Granite® is a family of open, performant and trusted AI models tailored for business and optimized to scale your AI applications. Explore language, code, time series and guardrail options.
Led by top IBM thought leaders, the curriculum is designed to help business leaders gain the knowledge needed to prioritize the AI investments that can drive growth.
Led by top IBM thought leaders, the curriculum is designed to help business leaders gain the knowledge needed to prioritize the AI investments that can drive growth.
We surveyed 2,000 organizations about their AI initiatives to discover what’s working, what’s not and how you can get ahead.
We surveyed 2,000 organizations about their AI initiatives to discover what’s working, what’s not and how you can get ahead.
Activate these five mindshifts to cut through the uncertainty, spur business reinvention, and supercharge growth with agentic AI.
Activate these five mindshifts to cut through the uncertainty, spur business reinvention, and supercharge growth with agentic AI.
Learn how to confidently incorporate generative AI and machine learning into your business.
Learn how to confidently incorporate generative AI and machine learning into your business.
Dive into the three critical elements of a strong AI strategy: creating a competitive edge, scaling AI across the business and advancing trustworthy AI.
Dive into the three critical elements of a strong AI strategy: creating a competitive edge, scaling AI across the business and advancing trustworthy AI.
Train, validate, tune and deploy generative AI, foundation models and machine learning capabilities with IBM watsonx.ai, a next-generation enterprise studio for AI builders. Build AI applications in a fraction of the time with a fraction of the data.
Put AI to work in your business with IBM’s industry-leading AI expertise and portfolio of solutions at your side.
IBM Consulting AI services help reimagine how businesses work with AI for transformation.
Get one-stop access to capabilities that span the AI development lifecycle. Produce powerful AI solutions with user-friendly interfaces, workflows and access to industry-standard APIs and SDKs.
Powered by IBM watsonx
Powered by IBM watsonx
- Account informationCurrentWe use your email to validate you are who you say you are, to create your IBMid, and to contact you for account related matters.Business emailYour subscription will be delivered in English. You will find an unsubscribe link in every newsletter. You can manage your subscriptions or unsubscribe  here. Refer to our  IBM Privacy Statement for more information.
- Account informationCurrent
- Artificial Narrow Intelligence (ANI)
- Artificial General Intelligence (AGI)
- Artificial Super Intelligence (ASI)

--------------------------------------------------

URL: https://www.geeksforgeeks.org/artificial-intelligence/difference-between-artificial-intelligence-vs-machine-learning-vs-deep-learning/
==================================================
Difference Between Artificial Intelligence vs Machine Learning vs Deep Learning
Artificial Intelligence is basically the mechanism to incorporate human intelligence into machines through a set of rules(algorithm). AI is a combination of two words: "Artificial" meaning something made by humans or non-natural things and "Intelligence" meaning the ability to understand or think accordingly. Another definition could be that "AI is basically the study of training your machine(computers) to mimic a human brain and its thinking capabilities".
AI focuses on 3 major aspects(skills): learning, reasoning, and self-correction to obtain the maximum efficiency possible.
Machine Learning is basically the study/process which provides the system(computer) to learn automatically on its own through experiences it had and improve accordingly without being explicitly programmed. ML is an application or subset of AI. ML focuses on the development of programs so that it can access data to use it for itself. The entire process makes observations on data to identify the possible patterns being formed and make better future decisions as per the examples provided to them. The major aim of ML is to allow the systems to learn by themselves through experience without any kind of human intervention or assistance.
Deep Learning is basically a sub-part of the broader family of Machine Learning which makes use of Neural Networks(similar to the neurons working in our brain) to mimic human brain-like behavior. DL algorithms focus on information processing patterns mechanism to possibly identify the patterns just like our human brain does and classifies the information accordingly. DL works on larger sets of data when compared to ML and the prediction mechanism is self-administered by machines.
Below is a table of differences between Artificial Intelligence, Machine Learning and Deep Learning:
AI vs. Machine Learning vs. Deep Learning Examples:
Artificial Intelligence (AI) refers to the development of computer systems that can perform tasks that would normally require human intelligence.
Some examples of AI include:
There are numerous examples of AI applications across various industries. Here are some common examples:
Examples of Machine Learning:
Machine Learning (ML) is a subset of Artificial Intelligence (AI) that involves the use of algorithms and statistical models to allow a computer system to "learn" from data and improve its performance over time, without being explicitly programmed to do so.
Here are some examples of Machine Learning:
Examples of Deep Learning:
Deep Learning is a type of Machine Learning that uses artificial neural networks with multiple layers to learn and make decisions.
Here are some examples of Deep Learning:
AI vs. ML vs. DL works: Is There a Difference?
Working in AI is not the same as being an ML or DL engineer. Here’s how you can tell those careers apart and decide which one is the right call for you.
What Does an AI Engineer Do?
An AI Engineer is a professional who designs, develops, and implements artificial intelligence (AI) systems and solutions. Here are some of the key responsibilities and tasks of an AI Engineer:
An AI Engineer must have a strong background in computer science, mathematics, and statistics, as well as experience in developing AI algorithms and solutions. They should also be familiar with programming languages, such as Python and R.
What Does a Machine Learning Engineer Do?
A Machine Learning Engineer is a professional who designs, develops, and implements machine learning (ML) systems and solutions. Here are some of the key responsibilities and tasks of a Machine Learning Engineer:
A Machine Learning Engineer must have a strong background in computer science, mathematics, and statistics, as well as experience in developing ML algorithms and solutions. They should also be familiar with programming languages, such as Python and R, and have experience working with ML frameworks and tools.
What Does a Deep Learning Engineer Do?
A Deep Learning Engineer is a professional who designs, develops, and implements deep learning (DL) systems and solutions. Here are some of the key responsibilities and tasks of a Deep Learning Engineer:
- Speech recognition: speech recognition systems use deep learning algorithms to recognize and classify images and speech. These systems are used in a variety of applications, such as self-driving cars, security systems, and medical imaging.
- Personalized recommendations: E-commerce sites and streaming services like Amazon and Netflix use AI algorithms to analyze users' browsing and viewing history to recommend products and content that they are likely to be interested in.
- Predictive maintenance: AI-powered predictive maintenance systems analyze data from sensors and other sources to predict when equipment is likely to fail, helping to reduce downtime and maintenance costs.
- Medical diagnosis: AI-powered medical diagnosis systems analyze medical images and other patient data to help doctors make more accurate diagnoses and treatment plans.
- Autonomous vehicles: Self-driving cars and other autonomous vehicles use AI algorithms and sensors to analyze their environment and make decisions about speed, direction, and other factors.
- Virtual Personal Assistants (VPA) like Siri or Alexa - these use natural language processing to understand and respond to user requests, such as playing music, setting reminders, and answering questions.
- Autonomous vehicles - self-driving cars use AI to analyze sensor data, such as cameras and lidar, to make decisions about navigation, obstacle avoidance, and route planning.
- Fraud detection - financial institutions use AI to analyze transactions and detect patterns that are indicative of fraud, such as unusual spending patterns or transactions from unfamiliar locations.
- Image recognition - AI is used in applications such as photo organization, security systems, and autonomous robots to identify objects, people, and scenes in images.
- Natural language processing - AI is used in chatbots and language translation systems to understand and generate human-like text.
- Predictive analytics - AI is used in industries such as healthcare and marketing to analyze large amounts of data and make predictions about future events, such as disease outbreaks or consumer behavior.
- Game-playing AI - AI algorithms have been developed to play games such as chess, Go, and poker at a superhuman level, by analyzing game data and making predictions about the outcomes of moves.
- Image recognition: Machine learning algorithms are used in image recognition systems to classify images based on their contents. These systems are used in a variety of applications, such as self-driving cars, security systems, and medical imaging.
- Speech recognition: Machine learning algorithms are used in speech recognition systems to transcribe speech and identify the words spoken. These systems are used in virtual assistants like Siri and Alexa, as well as in call centers and other applications.
- Natural language processing (NLP): Machine learning algorithms are used in NLP systems to understand and generate human language. These systems are used in chatbots, virtual assistants, and other applications that involve natural language interactions.
- Recommendation systems: Machine learning algorithms are used in recommendation systems to analyze user data and recommend products or services that are likely to be of interest. These systems are used in e-commerce sites, streaming services, and other applications.
- Sentiment analysis: Machine learning algorithms are used in sentiment analysis systems to classify the sentiment of text or speech as positive, negative, or neutral. These systems are used in social media monitoring and other applications.
- Predictive maintenance: Machine learning algorithms are used in predictive maintenance systems to analyze data from sensors and other sources to predict when equipment is likely to fail, helping to reduce downtime and maintenance costs.
- Spam filters in email - ML algorithms analyze email content and metadata to identify and flag messages that are likely to be spam.
- Recommendation systems - ML algorithms are used in e-commerce websites and streaming services to make personalized recommendations to users based on their browsing and purchase history.
- Predictive maintenance - ML algorithms are used in manufacturing to predict when machinery is likely to fail, allowing for proactive maintenance and reducing downtime.
- Credit risk assessment - ML algorithms are used by financial institutions to assess the credit risk of loan applicants, by analyzing data such as their income, employment history, and credit score.
- Customer segmentation - ML algorithms are used in marketing to segment customers into different groups based on their characteristics and behavior, allowing for targeted advertising and promotions.
- Fraud detection - ML algorithms are used in financial transactions to detect patterns of behavior that are indicative of fraud, such as unusual spending patterns or transactions from unfamiliar locations.
- Speech recognition - ML algorithms are used to transcribe spoken words into text, allowing for voice-controlled interfaces and dictation software.
- Image and video recognition: Deep learning algorithms are used in image and video recognition systems to classify and analyze visual data. These systems are used in self-driving cars, security systems, and medical imaging.
- Generative models: Deep learning algorithms are used in generative models to create new content based on existing data. These systems are used in image and video generation, text generation, and other applications.
- Autonomous vehicles: Deep learning algorithms are used in self-driving cars and other autonomous vehicles to analyze sensor data and make decisions about speed, direction, and other factors.
- Image classification - Deep Learning algorithms are used to recognize objects and scenes in images, such as recognizing faces in photos or identifying items in an image for an e-commerce website.
- Speech recognition - Deep Learning algorithms are used to transcribe spoken words into text, allowing for voice-controlled interfaces and dictation software.
- Natural language processing - Deep Learning algorithms are used for tasks such as sentiment analysis, language translation, and text generation.
- Recommender systems - Deep Learning algorithms are used in recommendation systems to make personalized recommendations based on users' behavior and preferences.
- Fraud detection - Deep Learning algorithms are used in financial transactions to detect patterns of behavior that are indicative of fraud, such as unusual spending patterns or transactions from unfamiliar locations.
- Game-playing AI - Deep Learning algorithms have been used to develop game-playing AI that can compete at a superhuman level, such as the AlphaGo AI that defeated the world champion in the game of Go.
- Time series forecasting - Deep Learning algorithms are used to forecast future values in time series data, such as stock prices, energy consumption, and weather patterns.
- Design and development of AI algorithms: AI Engineers design, develop, and implement AI algorithms, such as decision trees, random forests, and neural networks, to solve specific problems.
- Data analysis: AI Engineers analyze and interpret data, using statistical and mathematical techniques, to identify patterns and relationships that can be used to train AI models.
- Model training and evaluation: AI Engineers train AI models on large datasets, evaluate their performance, and adjust the parameters of the algorithms to improve accuracy.
- Deployment and maintenance: AI Engineers deploy AI models into production environments and maintain and update them over time.
- Collaboration with stakeholders: AI Engineers work closely with stakeholders, including data scientists, software engineers, and business leaders, to understand their requirements and ensure that the AI solutions meet their needs.
- Research and innovation: AI Engineers stay current with the latest advancements in AI and contribute to the research and development of new AI techniques and algorithms.
- Communication: AI Engineers communicate the results of their work, including the performance of AI models and their impact on business outcomes, to stakeholders.
- Design and development of ML algorithms: Machine Learning Engineers design, develop, and implement ML algorithms, such as decision trees, random forests, and neural networks, to solve specific problems.
- Data analysis: Machine Learning Engineers analyze and interpret data, using statistical and mathematical techniques, to identify patterns and relationships that can be used to train ML models.
- Model training and evaluation: Machine Learning Engineers train ML models on large datasets, evaluate their performance, and adjust the parameters of the algorithms to improve accuracy.
- Deployment and maintenance: Machine Learning Engineers deploy ML models into production environments and maintain and update them over time.
- Collaboration with stakeholders: Machine Learning Engineers work closely with stakeholders, including data scientists, software engineers, and business leaders, to understand their requirements and ensure that the ML solutions meet their needs.
- Research and innovation: Machine Learning Engineers stay current with the latest advancements in ML and contribute to the research and development of new ML techniques and algorithms.
- Communication: Machine Learning Engineers communicate the results of their work, including the performance of ML models and their impact on business outcomes, to stakeholders.
- Design and development of DL algorithms: Deep Learning Engineers design, develop, and implement deep neural networks and other DL algorithms to solve specific problems.
- Data analysis: Deep Learning Engineers analyze and interpret large datasets, using statistical and mathematical techniques, to identify patterns and relationships that can be used to train DL models.
- Model training and evaluation: Deep Learning Engineers train DL models on massive datasets, evaluate their performance, and adjust the parameters of the algorithms to improve accuracy.
- Deployment and maintenance: Deep Learning Engineers deploy DL models into production environments and maintain and update them over time.
- Collaboration with stakeholders: Deep Learning Engineers work closely with stakeholders, including data scientists, software engineers, and business leaders, to understand their requirements and ensure that the DL solutions meet their needs.
- Research and innovation: Deep Learning Engineers stay current with the latest advancements in DL and contribute to the research and development of new DL techniques and algorithms.
- Communication: Deep Learning Engineers communicate the results of their work, including the performance of DL models and their impact on business outcomes, to stakeholders.
- Artificial Intelligence

--------------------------------------------------

URL: https://www.mdpi.com/2075-4418/13/15/2582
==================================================
What Is Machine Learning, Artificial Neural Networks and Deep Learning?—Examples of Practical Applications in Medicine
3. Data and Its Relevance to Neural Networks
4. Machine Learning Models
4.1. Supervised Learning
4.2. Unsupervised Learning
5. Classical Methods of Machine Learning
5.1. k-Nearest Neighbour Algorithms
5.2. Linear Regression Algorithms
5.3. Logistic Regression Algorithms
5.4. Naive Bayes Classifier Algorithms
5.5. Support Vector Machines (SVMs)
7.1. Differences between the DNNs and ANNs
7.2. Deep Neural Network (DNN) Classifiers
7.3. Convolutional Neural Networks
7.4. Auto-Encoders (Unsupervised)
7.5. Segmenting Neural Networks (e.g., UNET and Lung Segmentation)
7.6. Generative Adversarial Networks
7.7. Transfer Learning
7.8. Few-Shot Learning
7.9. Deep Reinforcement Learning
7.10. Transformer Neural Networks
7.11. Attention Mechanism
8. Examples of AI Applications in Medicine Approved by the US Food and Drug Administration (FDA)
9. Discussion and Limitations
11. Summary and Future Research
Institutional Review Board Statement
Informed Consent Statement
Data Availability Statement
Conflicts of Interest
Kufel, J.;                     Bargieł-Łączek, K.;                     Kocot, S.;                     Koźlik, M.;                     Bartnikowska, W.;                     Janik, M.;                     Czogalik, Ł.;                     Dudek, P.;                     Magiera, M.;                     Lis, A.;
et al.    What Is Machine Learning, Artificial Neural Networks and Deep Learning?—Examples of Practical Applications in Medicine. Diagnostics 2023, 13, 2582.
https://doi.org/10.3390/diagnostics13152582
Kufel J,                                 Bargieł-Łączek K,                                 Kocot S,                                 Koźlik M,                                 Bartnikowska W,                                 Janik M,                                 Czogalik Ł,                                 Dudek P,                                 Magiera M,                                 Lis A,
et al.        What Is Machine Learning, Artificial Neural Networks and Deep Learning?—Examples of Practical Applications in Medicine. Diagnostics. 2023; 13(15):2582.
https://doi.org/10.3390/diagnostics13152582
Kufel, Jakub,                                 Katarzyna Bargieł-Łączek,                                 Szymon Kocot,                                 Maciej Koźlik,                                 Wiktoria Bartnikowska,                                 Michał Janik,                                 Łukasz Czogalik,                                 Piotr Dudek,                                 Mikołaj Magiera,                                 Anna Lis,
and et al.        2023. "What Is Machine Learning, Artificial Neural Networks and Deep Learning?—Examples of Practical Applications in Medicine" Diagnostics 13, no. 15: 2582.
https://doi.org/10.3390/diagnostics13152582
Kufel, J.,                                 Bargieł-Łączek, K.,                                 Kocot, S.,                                 Koźlik, M.,                                 Bartnikowska, W.,                                 Janik, M.,                                 Czogalik, Ł.,                                 Dudek, P.,                                 Magiera, M.,                                 Lis, A.,                                 Paszkiewicz, I.,                                 Nawrat, Z.,                                 Cebula, M.,                                 & Gruszczyńska, K.
(2023). What Is Machine Learning, Artificial Neural Networks and Deep Learning?—Examples of Practical Applications in Medicine. Diagnostics, 13(15), 2582.
https://doi.org/10.3390/diagnostics13152582
Article Access Statistics
- Model architecture—describes how the data is processed, transmitted, and analysed within the machine learning algorithm, which influences its efficiency and effectiveness in solving problems.
- Data exploration—the process of analysing and summarising a large dataset to gain insight into the relationships and patterns that exist within the data.
- Binary classification—a type of classification in which the aim is to assign one of two possible classes (labels) to an object: positive or negative, true or false, etc.
- Logistic function—a sigmoidal mathematical function that transforms values from minus infinity into plus infinity to the range (0, 1), allowing not only non-linearity, but also probability, e.g., binary classification.
- Input variables—also known as independent variables, explanatory variables, predictor variables, etc., and are variables that are used to describe or explain the behaviour, trends, or decisions of the target variables.
- Target variables—also known as dependent variables, outcome variables, etc., and are variables that are studied or predicted in statistical analysis and machine learning. Target variables are dependent on and are described using explanatory variables.
- Bayes theory—used to calculate the probability of an event, having prior information about that event.
- Sentiment analysis—the process of automatically determining emotions, opinions, and moods expressed in the text. This can be in the form of product reviews, comments on online forums, tweets on Twitter, or other forms of textual communication. The purpose of sentiment analysis is to gain an automatic understanding of whether a text is positive, negative, or neutral.
- Training—the process of formatting a model to interpret the data to perform a specific task with a specific accuracy. In this case, it is the determination of the hyperplane.
- Hyperplane—a set having n − 1 dimension, relative to the n-dimensional space in which it is contained (for n = two-dimensional space it has one dimension (point); for n = three-dimensional space it has two dimensions (line)).
- Training objects—a set of objects used to determine the hyperplane with the model.
- Support vectors—at least two objects at the shortest distance from the hyperplane belonging to two classes.
- Class—a group, described on numerical ranges, to which an object can be assigned—i.e., classified.
- Cluster—a hyperplane-limited space in a data system in which the presence of an object determines the class assignment.
- Neuron—the basic element of a neural network, which connects to other neurons through transmitting data to each other.
- Weight—the characteristic that the network designer provides to the connections between the neurons to achieve the desired results.
- Recursion—referring a function to the same function using the network being trained.
- Layer—a portion of the total network architecture that is differentiated from the other parts due to having a distinctive function.
- Activation function—takes the input from one or more neurons and maps it to an output value, which is then passed onto the next layer of neurons in the network.
- Hidden layer—in an artificial neural network, this is defined as the layer between the input and output layers, where the result of their action cannot be directly observed.
- Input layer—layer where the data are collected and passed onto the next layer.
- Output layer—layer which gathers the conclusions.
- Backpropagation—sending signals in the reverse order to calculate the error associated with each individual neuron in an effort to improve the model.
- Cost function—a function that represents the errors occurring in the training process in the form of a number. It is used for subsequent optimisation.
- Receptive field—a section of the image that is individually analysed using the filter.
- Filter—a set of numbers that are used to perform computational operations on the input data on splices. It is used to extract features (e.g., the presence of edges or curvature).
- Convolution—integral of the product of the two functions after one is reflected about the y-axis and shifted.
- Pooling—reducing the amount of data representing a given area of the image.
- Matrix—a mathematical concept; a set of numbers which is used, among other things, to recalculate the data obtained from neurons.
- Skip connections—a technique used in neural networks that allows information to be passed from one layer of the network to another, while skipping intermediate layers.
- Perceptron networks: simplest neural networks with an input and output layer composed of perceptrons. Perceptrons assign a value of one or zero based on the activation threshold, dividing the set into two.
- Layered networks (feed forward): multiple layers of interconnected neurons where the outputs of the previous layer neurons serve as the inputs for the next layer. The neurons of each successive layer always have a +1 input from the previous layer. Enables the classification of non-binary sets, and are used in image, text, and speech recognition.
- Recurrent networks: neural networks with feedback loops where the output signals feed back into the input neurons. Can generate sequences of phenomena and signals until the output stabilises. Used for sentiment analysis and text generation.
- Convolutional networks, also known as braided networks, are described in the next paragraph.
- Gated recurrent unit (GRU) and long short-term memory (LSTM) networks: perform recursive tasks with the output dependent on previous calculations. They have network memory, allowing them to remember data states across different time steps. These networks have longer training times and are applied in time series analysis (e.g., stock prices), autonomous car trajectory prediction, text-to-speech conversion, and language translation.
- Apple IRNF 2.0: Apple Inc. developed the IRNF 2.0 software for the Apple Watch, utilising CNNs and machine learning algorithms. This software aims to identify cardiac rhythm disorders, particularly atrial fibrillation (AFib). The study conducted by Apple in 2021, with FDA approval, involved over 2500 participants and collected more than 3 million heart rate recordings. The algorithm successfully differentiated between AFib and non-AFib rhythms, with a sensitivity of 88.6% and a specificity of 99.3%. While effective, it is important to note that the app does not replace professional diagnosis nor target individuals that have already been diagnosed with AFib [77].
- Ultromics: An AI-powered solution detects heart failure with a 90% accuracy, specifically heart failure with preserved ejection fraction (HFpEF). It analyses LV images using an AI ML-based algorithm to accurately measure LV parameters, such as volumes, left ventricular ejection fraction (LVEF), and left ventricular longitudinal strain (LVLS). This software (EchoGo Core 2.0) also classifies echocardiographic views for quality control. AI readings are more consistent than manual readings, regardless of the image quality. Additionally, AI-derived LVEF and LVLS values have been significantly associated with mortality in-hospital and at final follow-up [78].
- Aidoc: Aidoc is an AI-based platform that permits a fast and accurate analysis of X-rays and CT scans. It detects conditions, like strokes, fractures, and cancerous lesions. Their advanced AI softwares (Aidoc software from 2.0 and above) help radiologists to prioritise their critical cases and expedite patient care. Aidoc has 12 FDA-approved tools, including softwares (Aidoc software from 2.0 and above) for analysing head CT images (detecting intracranial haemorrhage), chest CT studies (identifying aortic dissection), chest X-rays (flagging a suspected pneumothorax), and abdominal CT images (indicating suspected intra-abdominal free gas) [79].
- Riverain Technologies: This AI-based platform enables an accurate and efficient analysis of lung images for detecting and monitoring lung conditions, like cancer, COPD, and bronchial disease. It includes features such as CT Detect for measuring areas of interest, ClearRead CT Compare for comparing nodules across studies, and ClearRead CT Vessel Suppress for enhancing nodule visibility. This patented technology improves the accuracy and reading performance, and seamlessly integrates processed series with the original CT series for synchronised scrolling [80].
- Ward, T.M.; Mascagni, P.; Madani, A.; Padoy, N.; Perretta, S.; Hashimoto, D.A. Surgical Data Science and Artificial Intelligence for Surgical Education. J. Surg. Oncol. 2021, 124, 221–230. [Google Scholar] [CrossRef]
- Mortani Barbosa, E.J.; Gefter, W.B.; Ghesu, F.C.; Liu, S.; Mailhe, B.; Mansoor, A.; Grbic, S.; Vogt, S. Automated Detection and Quantification of COVID-19 Airspace Disease on Chest Radiographs: A Novel Approach Achieving Expert Radiologist-Level Performance Using a Deep Convolutional Neural Network Trained on Digital Reconstructed Radiographs from Computed Tomography-Derived Ground Truth. Investig. Radiol. 2021, 56, 471–479. [Google Scholar] [CrossRef]
- Gupta, M. Introduction to Data in Machine Learning. GeeksforGeeks. Available online: https://www.geeksforgeeks.org/ml-introduction-data-machine-learning/ (accessed on 1 May 2023).
- Dorfman, E. How Much Data Is Required for Machine Learning? Postindustria. Available online: https://postindustria.com/how-much-data-is-required-for-machine-learning/ (accessed on 7 May 2023).
- Patel, H. Data-Centric Approach vs. Model-Centric Approach in Machine Learning. MLOps Blog 2023. Available online: https://neptune.ai/blog/data-centric-vs-model-centric-machine-learning (accessed on 1 May 2023).
- Brown, S. Machine Learning, Explained. MIT Sloan. Ideas Made to Matter. 2021. Available online: https://mitsloan.mit.edu/ideas-made-to-matter/machine-learning-explained (accessed on 4 May 2023).
- Christopher, A. K-Nearest Neighbor. Medium. The Startup. 2021. Available online: https://medium.com/swlh/k-nearest-neighbor-ca2593d7a3c4 (accessed on 10 May 2023).
- Hamed, A.; Sobhy, A.; Nassar, H. Accurate Classification of COVID-19 Based on Incomplete Heterogeneous Data Using a KNN Variant Algorithm. Arab J. Sci. Eng. 2021, 46, 8261–8272. [Google Scholar] [CrossRef]
- Bellino, G.; Schiaffino, L.; Battisti, M.; Guerrero, J.; Rosado-Muñoz, A. Optimization of the KNN Supervised Classification Algorithm as a Support Tool for the Implantation of Deep Brain Stimulators in Patients with Parkinson’s Disease. Entropy 2019, 21, 346. [Google Scholar] [CrossRef] [PubMed] [Green Version]
- What Is Linear Regression? IBM. Available online: https://www.ibm.com/topics/linear-regression (accessed on 11 May 2023).
- Garcia, J.M.V.; Bahloul, M.A.; Laleg-Kirati, T.-M. A Multiple Linear Regression Model for Carotid-to-Femoral Pulse Wave Velocity Estimation Based on Schrodinger Spectrum Characterization. In Proceedings of the 2022 44th Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC), Glasgow, UK, 11–15 July 2022; pp. 143–147. [Google Scholar]
- Co to Jest Uczenie Maszynowe? Microsoft Azure. Available online: https://azure.microsoft.com/pl-pl/resources/cloud-computing-dictionary/what-is-machine-learning-platform (accessed on 11 May 2023).
- Regresja Logistyczna. IBM. Available online: https://www.ibm.com/docs/pl/spss-statistics/28.0.0?topic=regression-logistic (accessed on 14 May 2023).
- Kleinbaum, D.G.; Klein, M. Logistic Regression. In Statistics for Biology and Health; Springer: New York, NY, USA, 2010; ISBN 978-1-4419-1741-6. [Google Scholar]
- Gruszczyński, M.; Witkowski, B.; Wiśniowski, A.; Szulc, A.; Owczarczuk, M.; Książek, M.; Bazyl, M. Mikroekonometria. Modele i Metody Analizy Danych Indywidualnych; Akademicka. Ekonomia; II; Wolters Kluwer Polska SA: Gdansk, Poland, 2012; ISBN 978-83-264-5184-3. [Google Scholar]
- Naiwny Klasyfikator Bayesa. StatSoft Internetowy Podręcznik Statystyki. Available online: https://www.statsoft.pl/textbook/stathome_stat.html?https%3A%2F%2Fwww.statsoft.pl%2Ftextbook%2Fgo_search.html%3Fq%3D%25bayersa (accessed on 2 May 2023).
- Možina, M.; Demšar, J.; Kattan, M.; Zupan, B. Nomograms for Visualization of Naive Bayesian Classifier. In Knowledge Discovery in Databases: PKDD 2004; Lecture Notes in Computer Science; Boulicaut, J.-F., Esposito, F., Giannotti, F., Pedreschi, D., Eds.;  Springer: Berlin/Heidelberg, Germany, 2004; Volume 3202, pp. 337–348. ISBN 978-3-540-23108-0. [Google Scholar]
- Minsky, M. Steps toward Artificial Intelligence. Proc. IRE 1961, 49, 8–30. [Google Scholar] [CrossRef]
- Zhou, S. Sparse SVM for Sufficient Data Reduction. IEEE Trans. Pattern Anal. Mach. Intell. 2021, 44, 5560–5571. [Google Scholar] [CrossRef] [PubMed]
- Bordes, A.; Ertekin, S.; Weston, J.; Bottou, L. Fast Kernel Classifiers with Online and Active Learning. J. Mach. Learn. 2005, 6, 1579–1619. [Google Scholar]
- Cortes, C.; Vapnik, V. Support-Vector Networks. Mach. Learn. 1995, 20, 273–297. [Google Scholar] [CrossRef]
- Noble, W.S. What Is a Support Vector Machine? Nat. Biotechnol. 2006, 24, 1565–1567. [Google Scholar] [CrossRef]
- Winters-Hilt, S.; Merat, S. SVM Clustering. BMC Bioinform. 2007, 8, S18. [Google Scholar] [CrossRef] [Green Version]
- Huang, S.; Cai, N.; Pacheco, P.P.; Narrandes, S.; Wang, Y.; Xu, W. Applications of Support Vector Machine (SVM) Learning in Cancer Genomics. Cancer Genom. Proteom. 2018, 15, 41–51. [Google Scholar] [CrossRef] [Green Version]
- Zhang, J.; Xu, J.; Hu, X.; Chen, Q.; Tu, L.; Huang, J.; Cui, J. Diagnostic Method of Diabetes Based on Support Vector Machine and Tongue Images. BioMed Res. Int. 2017, 2017, 7961494. [Google Scholar] [CrossRef] [PubMed] [Green Version]
- Schapire, R.E.; Freund, Y. Boosting: Foundations and Algorithms; Adaptive Computation and Machine Learning Series; MIT Press: Cambridge, MA, USA, 2012; ISBN 978-0-262-01718-3. [Google Scholar]
- Li, S.; Zeng, Y.; Chapman, W.C.; Erfanzadeh, M.; Nandy, S.; Mutch, M.; Zhu, Q. Adaptive Boosting (AdaBoost)-based Multiwavelength Spatial Frequency Domain Imaging and Characterization for Ex Vivo Human Colorectal Tissue Assessment. J. Biophotonics 2020, 13, e201960241. [Google Scholar] [CrossRef]
- Hatwell, J.; Gaber, M.M.; Atif Azad, R.M. Ada-WHIPS: Explaining AdaBoost Classification with Applications in the Health Sciences. BMC Med. Inform. Decis. Mak. 2020, 20, 250. [Google Scholar] [CrossRef] [PubMed]
- Baniasadi, A.; Rezaeirad, S.; Zare, H.; Ghassemi, M.M. Two-Step Imputation and AdaBoost-Based Classification for Early Prediction of Sepsis on Imbalanced Clinical Data. Crit. Care Med. 2021, 49, e91–e97. [Google Scholar] [CrossRef] [PubMed]
- Takemura, A.; Shimizu, A.; Hamamoto, K. Discrimination of Breast Tumors in Ultrasonic Images Using an Ensemble Classifier Based on the AdaBoost Algorithm with Feature Selection. IEEE Trans. Med. Imaging 2010, 29, 598–609. [Google Scholar] [CrossRef]
- Salcedo-Sanz, S.; Pérez-Aracil, J.; Ascenso, G.; Del Ser, J.; Casillas-Pérez, D.; Kadow, C.; Fister, D.; Barriopedro, D.; García-Herrera, R.; Restelli, M.; et al. Analysis, Characterization, Prediction and Attribution of Extreme Atmospheric Events with Machine Learning: A Review. arXiv 2022, arXiv:2207.07580. [Google Scholar] [CrossRef]
- Moore, A.; Bell, M. XGBoost, A Novel Explainable AI Technique, in the Prediction of Myocardial Infarction: A UK Biobank Cohort Study. Clin. Med. Insights Cardiol. 2022, 16, 117954682211336. [Google Scholar] [CrossRef]
- Wang, X.; Zhu, T.; Xia, M.; Liu, Y.; Wang, Y.; Wang, X.; Zhuang, L.; Zhong, D.; Zhu, J.; He, H.; et al. Predicting the Prognosis of Patients in the Coronary Care Unit: A Novel Multi-Category Machine Learning Model Using XGBoost. Front. Cardiovasc. Med. 2022, 9, 764629. [Google Scholar] [CrossRef]
- Subha Ramakrishnan, M.; Ganapathy, N. Extreme Gradient Boosting Based Improved Classification of Blood-Brain-Barrier Drugs. In Studies in Health Technology and Informatics; Séroussi, B., Weber, P., Dhombres, F., Grouin, C., Liebe, J.-D., Pelayo, S., Pinna, A., Rance, B., Sacchi, L., Ugon, A., et al., Eds.;  IOS Press: Amsterdam, The Netherlands, 2022; ISBN 978-1-64368-284-6. [Google Scholar]
- Inoue, T.; Ichikawa, D.; Ueno, T.; Cheong, M.; Inoue, T.; Whetstone, W.D.; Endo, T.; Nizuma, K.; Tominaga, T. XGBoost, a Machine Learning Method, Predicts Neurological Recovery in Patients with Cervical Spinal Cord Injury. Neurotrauma Rep. 2020, 1, 8–16. [Google Scholar] [CrossRef]
- Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A.N.; Kaiser, L.; Polosukhin, I. Attention Is All You Need. In Proceedings of the NIPS 2017, Long Beach, CA, USA, 4–9 December 2017. [Google Scholar] [CrossRef]
- Shao, R.; Shi, Z.; Yi, J.; Chen, P.-Y.; Hsieh, C.-J. On the Adversarial Robustness of Vision Transformers. arXiv 2021, arXiv:2103.15670. [Google Scholar] [CrossRef]
- Qureshi, J. What Is the Difference between Neural Networks and Deep Neural Networks? Quora 2018. Available online: https://www.quora.com/What-is-the-difference-between-neural-networks-and-deep-neural-networks (accessed on 3 May 2023).
- Jeffrey, C. Explainer: What Is Machine Learning? TechSpot 2020. Available online: https://www.techspot.com/article/2048-machine-learning-explained/ (accessed on 3 May 2023).
- McBee, M.P.; Awan, O.A.; Colucci, A.T.; Ghobadi, C.W.; Kadom, N.; Kansagra, A.P.; Tridandapani, S.; Auffermann, W.F. Deep Learning in Radiology. Acad. Radiol. 2018, 25, 1472–1480. [Google Scholar] [CrossRef] [PubMed] [Green Version]
- Chan, H.-P.; Samala, R.K.; Hadjiiski, L.M.; Zhou, C. Deep Learning in Medical Image Analysis. In Deep Learning in Medical Image Analysis; Advances in Experimental Medicine and Biology; Lee, G., Fujita, H., Eds.;  Springer International Publishing: Cham, Switzerland, 2020; Volume 1213, pp. 3–21. ISBN 978-3-030-33127-6. [Google Scholar]
- Kriegeskorte, N.; Golan, T. Neural Network Models and Deep Learning. Curr. Biol. 2019, 29, R231–R236. [Google Scholar] [CrossRef] [PubMed]
- Bajić, F.; Orel, O.; Habijan, M. A Multi-Purpose Shallow Convolutional Neural Network for Chart Images. Sensors 2022, 22, 7695. [Google Scholar] [CrossRef]
- Han, R.; Yang, Y.; Li, X.; Ouyang, D. Predicting Oral Disintegrating Tablet Formulations by Neural Network Techniques. Asian J. Pharm. Sci. 2018, 13, 336–342. [Google Scholar] [CrossRef]
- Egger, J.; Gsaxner, C.; Pepe, A.; Pomykala, K.L.; Jonske, F.; Kurz, M.; Li, J.; Kleesiek, J. Medical Deep Learning—A Systematic Meta-Review. Comput. Methods Programs Biomed. 2022, 221, 106874. [Google Scholar] [CrossRef]
- Jafari, R.; Spincemaille, P.; Zhang, J.; Nguyen, T.D.; Luo, X.; Cho, J.; Margolis, D.; Prince, M.R.; Wang, Y. Deep Neural Network for Water/Fat Separation: Supervised Training, Unsupervised Training, and No Training. Magn. Reson. Med. 2021, 85, 2263–2277. [Google Scholar] [CrossRef]
- Hou, J.-J.; Tian, H.-L.; Lu, B. A Deep Neural Network-Based Model for Quantitative Evaluation of the Effects of Swimming Training. Comput. Intell. Neurosci. 2022, 2022, 5508365. [Google Scholar] [CrossRef]
- Singh, A.; Ardakani, A.A.; Loh, H.W.; Anamika, P.V.; Acharya, U.R.; Kamath, S.; Bhat, A.K. Automated Detection of Scaphoid Fractures Using Deep Neural Networks in Radiographs. Eng. Appl. Artif. Intell. 2023, 122, 106165. [Google Scholar] [CrossRef]
- Gülmez, B. A Novel Deep Neural Network Model Based Xception and Genetic Algorithm for Detection of COVID-19 from X-Ray Images. Ann. Oper. Res. 2022.  [CrossRef]
- Tsai, K.-J.; Chou, M.-C.; Li, H.-M.; Liu, S.-T.; Hsu, J.-H.; Yeh, W.-C.; Hung, C.-M.; Yeh, C.-Y.; Hwang, S.-H. A High-Performance Deep Neural Network Model for BI-RADS Classification of Screening Mammography. Sensors 2022, 22, 1160. [Google Scholar] [CrossRef]
- Sharrock, M.F.; Mould, W.A.; Ali, H.; Hildreth, M.; Awad, I.A.; Hanley, D.F.; Muschelli, J. 3D Deep Neural Network Segmentation of Intracerebral Hemorrhage: Development and Validation for Clinical Trials. Neuroinform 2021, 19, 403–415. [Google Scholar] [CrossRef] [PubMed]
- Jiao, Y.; Yuan, J.; Sodimu, O.M.; Qiang, Y.; Ding, Y. Deep Neural Network-Aided Histopathological Analysis of Myocardial Injury. Front. Cardiovasc. Med. 2022, 8, 724183. [Google Scholar] [CrossRef]
- Rajput, J.S.; Sharma, M.; Kumar, T.S.; Acharya, U.R. Automated Detection of Hypertension Using Continuous Wavelet Transform and a Deep Neural Network with Ballistocardiography Signals. IJERPH 2022, 19, 4014. [Google Scholar] [CrossRef] [PubMed]
- Voigt, I.; Boeckmann, M.; Bruder, O.; Wolf, A.; Schmitz, T.; Wieneke, H. A Deep Neural Network Using Audio Files for Detection of Aortic Stenosis. Clin. Cardiol. 2022, 45, 657–663. [Google Scholar] [CrossRef]
- Ma, L.; Yang, T. Construction and Evaluation of Intelligent Medical Diagnosis Model Based on Integrated Deep Neural Network. Comput. Intell. Neurosci. 2021, 2021, 7171816. [Google Scholar] [CrossRef]
- Ragab, M.; AL-Ghamdi, A.S.A.-M.; Fakieh, B.; Choudhry, H.; Mansour, R.F.; Koundal, D. Prediction of Diabetes through Retinal Images Using Deep Neural Network. Comput. Intell. Neurosci. 2022, 2022, 7887908. [Google Scholar] [CrossRef]
- Min, J.K.; Yang, H.-J.; Kwak, M.S.; Cho, C.W.; Kim, S.; Ahn, K.-S.; Park, S.-K.; Cha, J.M.; Park, D.I. Deep Neural Network-Based Prediction of the Risk of Advanced Colorectal Neoplasia. Gut Liver 2021, 15, 85–91. [Google Scholar] [CrossRef] [PubMed]
- Anwar, S.M.; Majid, M.; Qayyum, A.; Awais, M.; Alnowami, M.; Khan, M.K. Medical Image Analysis Using Convolutional Neural Networks: A Review. J. Med. Syst. 2018, 42, 226. [Google Scholar] [CrossRef] [Green Version]
- Mohamed, E.A.; Gaber, T.; Karam, O.; Rashed, E.A. A Novel CNN Pooling Layer for Breast Cancer Segmentation and Classification from Thermograms. PLoS ONE 2022, 17, e0276523. [Google Scholar] [CrossRef]
- Chamberlin, J.; Kocher, M.R.; Waltz, J.; Snoddy, M.; Stringer, N.F.C.; Stephenson, J.; Sahbaee, P.; Sharma, P.; Rapaka, S.; Schoepf, U.J.; et al. Automated Detection of Lung Nodules and Coronary Artery Calcium Using Artificial Intelligence on Low-Dose CT Scans for Lung Cancer Screening: Accuracy and Prognostic Value. BMC Med. 2021, 19, 55. [Google Scholar] [CrossRef] [PubMed]
- Alajanbi, M.; Malerba, D.; Liu, H. Distributed Reduced Convolution Neural Networks. Mesopotamian J. Big Data 2021, 2021, 26–29. [Google Scholar] [CrossRef]
- Le, Q.V. A Tutorial on Deep Learning Part 2: Autoencoders, Convolutional Neural Networks and Recurrent Neural Networks. Google Brain 2015, 20, 1–20. [Google Scholar]
- Ren, L.; Sun, Y.; Cui, J.; Zhang, L. Bearing Remaining Useful Life Prediction Based on Deep Autoencoder and Deep Neural Networks. J. Manuf. Syst. 2018, 48, 71–77. [Google Scholar] [CrossRef]
- Dertat, A. Applied Deep Learning-Part 3: Autoencoders. Medium. Towards Data Science. 2017. Available online: https://towardsdatascience.com/applied-deep-learning-part-3-autoencoders-1c083af4d798 (accessed on 3 May 2023).
- Baur, C.; Denner, S.; Wiestler, B.; Navab, N.; Albarqouni, S. Autoencoders for Unsupervised Anomaly Segmentation in Brain MR Images: A Comparative Study. Med. Image Anal. 2021, 69, 101952. [Google Scholar] [CrossRef]
- Cao, H.; Wang, Y.; Chen, J.; Jiang, D.; Zhang, X.; Tian, Q.; Wang, M. Swin-Unet: Unet-like Pure Transformer for Medical Image Segmentation. arXiv 2022, arXiv:2105.05537. [Google Scholar] [CrossRef]
- Shamim, S.; Awan, M.J.; Mohd Zain, A.; Naseem, U.; Mohammed, M.A.; Garcia-Zapirain, B. Automatic COVID-19 Lung Infection Segmentation through Modified Unet Model. J. Healthc. Eng. 2022, 2022, 6566982. [Google Scholar] [CrossRef]
- Zhang, A.; Wang, H.; Li, S.; Cui, Y.; Liu, Z.; Yang, G.; Hu, J. Transfer Learning with Deep Recurrent Neural Networks for Remaining Useful Life Estimation. Appl. Sci. 2018, 8, 2416. [Google Scholar] [CrossRef] [Green Version]
- Rios, A.; Kavuluru, R. Neural Transfer Learning for Assigning Diagnosis Codes to EMRs. Artif. Intell. Med. 2019, 96, 116–122. [Google Scholar] [CrossRef]
- Snell; Swersky, K.; Zemel, R.S. Prototypical Networks for Few-Shot Learning. In Proceedings of the NIPS 2017, Long Beach, CA, USA, 4–9 December 2017. [Google Scholar] [CrossRef]
- Wang, Y.; Wu, X.-M.; Li, Q.; Gu, J.; Xiang, W.; Zhang, L.; Li, V.O.K. Large Margin Few-Shot Learning. arXiv 2018, arXiv:1807.02872. [Google Scholar]
- Berger, M.; Yang, Q.; Maier, A. X-ray Imaging. In Medical Imaging Systems; Lecture Notes in Computer Science; Maier, A., Steidl, S., Christlein, V., Hornegger, J., Eds.;  Springer International Publishing: Cham, Switzerland, 2018; Volume 11111, pp. 119–145. ISBN 978-3-319-96519-2. [Google Scholar]
- Nie, M.; Chen, D.; Wang, D. Reinforcement Learning on Graphs: A Survey. arXiv 2022, arXiv:2204.06127. [Google Scholar] [CrossRef]
- Giacaglia, G. How Transformers Work. The Neural Network Used by Open AI and DeepMind. Towards Data Science 2019. Available online: https://towardsdatascience.com/transformers-141e32e69591 (accessed on 1 May 2023).
- Luo, X.; Gandhi, P.; Zhang, Z.; Shao, W.; Han, Z.; Chandrasekaran, V.; Turzhitsky, V.; Bali, V.; Roberts, A.R.; Metzger, M.; et al. Applying Interpretable Deep Learning Models to Identify Chronic Cough Patients Using EHR Data. Comput. Methods Programs Biomed. 2021, 210, 106395. [Google Scholar] [CrossRef] [PubMed]
- Li, L.; Zhao, J.; Hou, L.; Zhai, Y.; Shi, J.; Cui, F. An Attention-Based Deep Learning Model for Clinical Named Entity Recognition of Chinese Electronic Medical Records. BMC Med. Inform. Decis. Mak. 2019, 19, 235. [Google Scholar] [CrossRef] [PubMed] [Green Version]
- Available online. Available online: https://www.accessdata.fda.gov/cdrh_docs/pdf21/K212516.pdf (accessed on 5 May 2023).
- Asch, F.M.; Descamps, T.; Sarwar, R.; Karagodin, I.; Singulane, C.C.; Xie, M.; Tucay, E.S.; Tude Rodrigues, A.C.; Vasquez-Ortiz, Z.Y.; Monaghan, M.J.; et al. Human versus Artificial Intelligence–Based Echocardiographic Analysis as a Predictor of Outcomes: An Analysis from the World Alliance Societies of Echocardiography COVID Study. J. Am. Soc. Echocardiogr. 2022, 35, 1226–1237.e7. [Google Scholar] [CrossRef]
- Aidoc. Available online: https://www.aidoc.com/solutions/radiology/ (accessed on 5 May 2023).
- Riverain Technologies. Available online: https://www.riveraintech.com/clearread-ai-solutions/clearread-ct/ (accessed on 5 May 2023).
- Alzubaidi, L.; Bai, J.; Al-Sabaawi, A.; Santamaría, J.; Albahri, A.S.; Al-dabbagh, B.S.N.; Fadhel, M.A.; Manoufali, M.; Zhang, J.; Al-Timemy, A.H.; et al. A Survey on Deep Learning Tools Dealing with Data Scarcity: Definitions, Challenges, Solutions, Tips, and Applications. J. Big Data 2023, 10, 46. [Google Scholar] [CrossRef]
- Albahri, A.S.; Duhaim, A.M.; Fadhel, M.A.; Alnoor, A.; Baqer, N.S.; Alzubaidi, L.; Albahri, O.S.; Alamoodi, A.H.; Bai, J.; Salhi, A.; et al. A Systematic Review of Trustworthy and Explainable Artificial Intelligence in Healthcare: Assessment of Quality, Bias Risk, and Data Fusion. Inf. Fusion 2023, 96, 156–191. [Google Scholar] [CrossRef]
- Hephzipah, J.J.; Vallem, R.R.; Sheela, M.S.; Dhanalakshmi, G. An Efficient Cyber Security System Based on Flow-Based Anomaly Detection Using Artificial Neural Network. Mesopotamian J. Cybersecur. 2023, 2023, 48–56. [Google Scholar] [CrossRef]
- Oliver, M.; Renou, A.; Allou, N.; Moscatelli, L.; Ferdynus, C.; Allyn, J. Image Augmentation and Automated Measurement of Endotracheal-Tube-to-Carina Distance on Chest Radiographs in Intensive Care Unit Using a Deep Learning Model with External Validation. Crit. Care 2023, 27, 40. [Google Scholar] [CrossRef]
- Moon, J.-H.; Hwang, H.-W.; Yu, Y.; Kim, M.-G.; Donatelli, R.E.; Lee, S.-J. How Much Deep Learning Is Enough for Automatic Identification to Be Reliable? Angle Orthod. 2020, 90, 823–830. [Google Scholar] [CrossRef]
- Albrecht, T.; Slabaugh, G.; Alonso, E.; Al-Arif, S.M.R. Deep Learning for Single-Molecule Science. Nanotechnology 2017, 28, 423001. [Google Scholar] [CrossRef]
- Yang, Z.; Zhang, A.; Sudjianto, A. Enhancing Explainability of Neural Networks Through Architecture Constraints. IEEE Trans. Neural Netw. Learn. Syst. 2021, 32, 2610–2621. [Google Scholar] [CrossRef] [PubMed]
- Stock, P.; Cisse, M. ConvNets and ImageNet Beyond Accuracy: Understanding Mistakes and Uncovering Biases. arXiv 2017, arXiv:1711.11443. [Google Scholar] [CrossRef]
- Zheng, Q.; Yang, L.; Zeng, B.; Li, J.; Guo, K.; Liang, Y.; Liao, G. Artificial Intelligence Performance in Detecting Tumor Metastasis from Medical Radiology Imaging: A Systematic Review and Meta-Analysis. EClinicalMedicine 2021, 31, 100669. [Google Scholar] [CrossRef] [PubMed]
- Ahmad, Z.; Rahim, S.; Zubair, M.; Abdul-Ghafar, J. Artificial Intelligence (AI) in Medicine, Current Applications and Future Role with Special Emphasis on Its Potential and Promise in Pathology: Present and Future Impact, Obstacles Including Costs and Acceptance among Pathologists, Practical and Philosophical Considerations. A Comprehensive Review. Diagn. Pathol. 2021, 16, 24. [Google Scholar] [CrossRef]
- Syed, A.; Zoga, A. Artificial Intelligence in Radiology: Current Technology and Future Directions. Semin. Musculoskelet. Radiol. 2018, 22, 540–545. [Google Scholar] [CrossRef]
- McDougall, R.J. Computer Knows Best? The Need for Value-Flexibility in Medical AI. J. Med. Ethics 2019, 45, 156–160. [Google Scholar] [CrossRef]
- Dave, M.; Patel, N. Artificial Intelligence in Healthcare and Education. Br. Dent. J. 2023, 234, 761–764. [Google Scholar] [CrossRef]

--------------------------------------------------

URL: https://www.sciencedirect.com/science/article/pii/B9780443221323000022
==================================================
- Reference number: 968d27a30a8c755c
- IP Address: 103.215.237.23
- User Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36
- Timestamp: 2025-08-02 11:14:59 UTC

--------------------------------------------------

URL: https://eicta.iitk.ac.in/knowledge-hub/artificial-intelligence/ai-vs-ml-vs-deep-learning-vs-genai/
==================================================
AI vs ML vs Dl and GenAI have become the most talked-about technology models. The way these technologies have integrated and made our lives easier still feels so fresh.
Yet, many people still confuse themselves when trying to understand their distinctions and interconnections with each other. What sets them apart? How do they work together?
In this blog, we have cleared the air on the difference between AI, ML, DL, and generative AI.
With this guide, you will gain a better understanding of each of them, their scope, and future trends in 2025.
AI is abbreviated for Artificial Intelligence. It stimulates human intelligence to perform tasks that typically require human cognition, like problem-solving, decision-making, and learning. It allows systems to think, learn, and solve problems without being explicitly programmed for every task.
Do you want to learn Artificial Intelligence in simple words – Click Here!!
Examples of AI in 2025:
Applications of AI in Modern Industries:
ML stands for Machine Learning, which is, in fact, a subset of AI. It allows systems to learn patterns and make predictions without manual programming. However, it relies on some algorithms to make those decisions. They are:
Applications of ML in Industries 2025:
Interested in E&ICT courses? Get a callback !
Best Artificial Intelligence and Machine Learning Course 2025
Artificial Intelligence (AI) and Machine Learning (ML) are not just buzzwords—they are the driving forces behind the world’s most significant advancements today. From self-driving cars to personalized healthcare solutions, AI and ML are reshaping every industry, creating a massive demand for skilled professionals.
If you’re looking to future-proof your career, this is your moment. Our comprehensive AI and ML course is designed to take you from beginner to expert, equipping you with practical skills in Python, data science, deep learning, and more. Join our best AI and ML Online Programs today to master the technologies that are set to dominate the future and become a leader in this high-growth field.
Best Artificial Intelligence and Machine Learning Online Course with Certification – Enroll Today!!
What’s the wait for? Enroll now and step into the world of the AI revolution.
What is Deep Learning?
Deep learning falls under machine learning, which is itself a part of AI. But mostly, it is a foundation for GEN AI and LLMs. So, what makes deep learning different from ML? Unlike ML’s traditional algorithms that need to be extracted, deep learning uses neural networks that are inspired by the human brain to make decisions. These neural nodes of deep learning help
Key Applications of Deep Learning:
What is Generative AI?
Generative AI is a modern form of AI. It is designed to create new content—text, images, videos, and music. In fact, the future of generative AI sees more opportunities in fields of media and designing.
Applications of GenAI in Industries:
What is the Difference Between AI, ML, DL, and Generative AI?
Machines mimicking human intelligence.
Algorithms learning patterns from data.
Neural networks simulating human brain processing.
Generative AI creating new content like text and images.
Automation and decision-making.
Pattern recognition and predictions.
Complex tasks like image/speech recognition.
Content creation and creative outputs.
Specialised subset for content generation.
Alexa, self-driving cars.
Netflix recommendations, fraud detection.
Facial recognition, autonomous driving.
AI art tools, ChatGPT, DALL-E.
This comparison clarifies what is the difference between AI ML DL and Generative AI, helping readers understand their distinct roles.
Future Trends in AI and ML Technologies
While AI is evolving every moment, here’s what you can probably expect as a trend for AI and ML in 2025:
1. Explainable AI (XAI)
While AI gives prompt responses, but what is the reasoning behind this decision? Users now demand transparency in AI decisions. This is where Explainable AI, aka XAI, helps them by explaining why the AI model came to a certain decision.
Explaining decision trees or linear regression models can be easy, but deep learning models operate as “black boxes.” Their internal workings are not easily interpretable. XAI techniques will evolve to demystify these models so that even a non-expert can understand.
With the increasing use of AI in every sector, there are growing concerns about Cybersecurity and data compliance. Therefore, the market for AI governance is also on a growing trend, with a projected CAGR of 35.7% by 2030.
The next few years will see increased focus on ethical AI and privacy laws.
3. AI in Media and Films
Generative AI will continue to disrupt industries by automating design, music, and video production. So basically, it’s, in a way, helping make creative tasks faster and easier.
Making virtual avatars for movies is now possible with AI. Earlier, it would have taken a lot of investment and time to get results, but now production houses are also relying on GenAI. The future is near where Generative AI will produce all movies. Possibility of AI-generated actors and voiceovers for animations.
4. Advanced Personalisation Engines
Engines and Platforms are already using AI to give personalized experiences to users. Be it your Netflix recommendations, your Spotify playlists based on your past viewing or listening habits, or your Amazon “also buy” recommendations, AI is offering a personal touch to make the experience feel curated and special.
But in 2025, we can expect hyper-personalization in terms of –
Understanding what is the difference between AI ML DL and Generative AI is essential as these technologies shape our future.
With 2025 poised for breakthroughs, mastering these technologies can open endless possibilities in a career as well.
Top Artificial Intelligence Techniques in 2025
How to Choose the Right CTO Program for Your Professional Goals in 2025
Prompt Engineering Best Practices in 2025: Safe AI Prompting for Developers & Analysts
What is Prompt Engineering: The AI Skill You Need in 2025
ML or AI: What Should I Learn in 2025
Leave A Reply Cancel reply
Your email address will not be published. Required fields are marked *
- Self-driving Cars – Auto vehicles by Tesla and Waymo are very common in foreign countries.
- AI-Powered Chatbots – Tools like ChatGPT, Gemini, and Google Bard are no longer gatekept.
- Healthcare Diagnostics – AI-based medical imaging for early disease detection.
- Virtual Assistants – Devices like Alexa and Siri improve daily convenience.
- Creative Industry
- Decision Trees
- Random Forest
- Linear Regression
- Neural Networks
- Logistic Regression
- It learns from data to improve performance.
- Adapts to changes over time.
- Automates repetitive tasks.
- Supply Chain
- Media and Entertainment
- United States+1
- United Kingdom+44
- Afghanistan+93
- Albania+355
- Algeria+213
- American Samoa+1
- Andorra+376
- Antigua & Barbuda+1
- Argentina+54
- Armenia+374
- Ascension Island+247
- Australia+61
- Azerbaijan+994
- Bahrain+973
- Bangladesh+880
- Belarus+375
- Bolivia+591
- Bosnia & Herzegovina+387
- Botswana+267
- British Indian Ocean Territory+246
- British Virgin Islands+1
- Bulgaria+359
- Burkina Faso+226
- Burundi+257
- Cambodia+855
- Cameroon+237
- Cape Verde+238
- Caribbean Netherlands+599
- Cayman Islands+1
- Central African Republic+236
- Christmas Island+61
- Cocos (Keeling) Islands+61
- Colombia+57
- Comoros+269
- Congo - Brazzaville+242
- Congo - Kinshasa+243
- Cook Islands+682
- Costa Rica+506
- Croatia+385
- Curaçao+599
- Czechia+420
- Côte d’Ivoire+225
- Djibouti+253
- Dominican Republic+1
- Ecuador+593
- El Salvador+503
- Equatorial Guinea+240
- Eritrea+291
- Estonia+372
- Eswatini+268
- Ethiopia+251
- Falkland Islands+500
- Faroe Islands+298
- Finland+358
- French Guiana+594
- French Polynesia+689
- Georgia+995
- Gibraltar+350
- Greenland+299
- Guadeloupe+590
- Guatemala+502
- Guernsey+44
- Guinea-Bissau+245
- Honduras+504
- Hong Kong SAR China+852
- Iceland+354
- Indonesia+62
- Ireland+353
- Isle of Man+44
- Kazakhstan+7
- Kiribati+686
- Kyrgyzstan+996
- Lebanon+961
- Lesotho+266
- Liberia+231
- Liechtenstein+423
- Lithuania+370
- Luxembourg+352
- Macao SAR China+853
- Madagascar+261
- Malaysia+60
- Maldives+960
- Marshall Islands+692
- Martinique+596
- Mauritania+222
- Mauritius+230
- Mayotte+262
- Micronesia+691
- Moldova+373
- Mongolia+976
- Montenegro+382
- Montserrat+1
- Morocco+212
- Mozambique+258
- Myanmar (Burma)+95
- Namibia+264
- Netherlands+31
- New Caledonia+687
- New Zealand+64
- Nicaragua+505
- Nigeria+234
- Norfolk Island+672
- North Korea+850
- North Macedonia+389
- Northern Mariana Islands+1
- Pakistan+92
- Palestinian Territories+970
- Papua New Guinea+675
- Paraguay+595
- Philippines+63
- Portugal+351
- Puerto Rico+1
- Réunion+262
- San Marino+378
- Saudi Arabia+966
- Senegal+221
- Seychelles+248
- Sierra Leone+232
- Singapore+65
- Sint Maarten+1
- Slovakia+421
- Slovenia+386
- Solomon Islands+677
- Somalia+252
- South Africa+27
- South Korea+82
- South Sudan+211
- Sri Lanka+94
- St. Barthélemy+590
- St. Helena+290
- St. Kitts & Nevis+1
- St. Lucia+1
- St. Martin+590
- St. Pierre & Miquelon+508
- St. Vincent & Grenadines+1
- Suriname+597
- Svalbard & Jan Mayen+47
- Switzerland+41
- São Tomé & Príncipe+239
- Tajikistan+992
- Tanzania+255
- Thailand+66
- Timor-Leste+670
- Tokelau+690
- Trinidad & Tobago+1
- Tunisia+216
- Turkmenistan+993
- Turks & Caicos Islands+1
- U.S. Virgin Islands+1
- Ukraine+380
- United Arab Emirates+971
- United Kingdom+44
- United States+1
- Uruguay+598
- Uzbekistan+998
- Vanuatu+678
- Vatican City+39
- Venezuela+58
- Wallis & Futuna+681
- Western Sahara+212
- Zimbabwe+263
- Åland Islands+358
- We offer self-paced online courses that you can complete within 40 hours.
- You gain an in-depth understanding of the subject matter.
- All programs are certified by IIT Kanpur.
- Comes with doubt sessions and at least 2 master classes.
- Processes complex data using multiple layers.
- Handles large datasets for precise decisions.
- Requires massive computing power.
- Speech Recognition – Google Assistant and Siri depend on DL for voice processing.
- Image Recognition – Used in facial recognition technologies.
- Autonomous Driving – Self-driving cars rely on DL algorithms for navigation
- Generates realistic and creative outputs.
- Learns patterns to produce human-like content.
- Enhances art, music, and even virtual avatars.
- Entertainment
- Emotionally Intelligent Interactions – Future AI systems will detect and respond to user emotions.
- Predictive Personalization – To know the probable needs of users even before they know or the need arises.
- Real-time Recommendations: AI will analyze user behavior at the moment and match your current interests and actions.
- AI mimics human intelligence.
- ML learns from patterns.
- DL processes complex tasks through neural networks.
- Generative AI creates innovative content.

--------------------------------------------------

URL: https://www.ibm.com/think/topics/artificial-intelligence
==================================================
What is artificial intelligence (AI)?
Editorial Lead, AI Models
Artificial intelligence (AI) is technology that enables computers and machines to simulate human learning, comprehension, problem solving, decision making, creativity and autonomy.
Applications and devices equipped with AI can see and identify objects. They can understand and respond to human language. They can learn from new information and experience. They can make detailed recommendations to users and experts. They can act independently, replacing the need for human intelligence or intervention (a classic example being a self-driving car).
But in 2024, most AI researchers, practitioners and most AI-related headlines are focused on breakthroughs in generative AI (gen AI), a technology that can create original text, images, video and other content. To fully understand generative AI, it’s important to first understand the technologies on which generative AI tools are built: machine learning (ML) and deep learning.
The latest tech news, backed by expert insights
Stay up to date on the most important—and intriguing—industry trends on AI, automation, data and beyond with the Think newsletter. See the IBM Privacy Statement.
Thank you! You are subscribed.
Your subscription will be delivered in English. You will find an unsubscribe link in every newsletter. You can manage your subscriptions or unsubscribe  here. Refer to our  IBM Privacy Statement for more information.
Your subscription will be delivered in English. You will find an unsubscribe link in every newsletter. You can manage your subscriptions or unsubscribe here. Refer to our IBM Privacy Statement for more information.
A simple way to think about AI is as a series of nested or derivative concepts that have emerged over more than 70 years:
Directly underneath AI, we have machine learning, which involves creating models by training an algorithm to make predictions or decisions based on data. It encompasses a broad range of techniques that enable computers to learn from and make inferences based on data without being explicitly programmed for specific tasks.
There are many types of machine learning techniques or algorithms, including linear regression, logistic regression, decision trees, random forest, support vector machines (SVMs), k-nearest neighbor (KNN), clustering and more. Each of these approaches is suited to different kinds of problems and data.
But one of the most popular types of machine learning algorithm is called a neural network (or artificial neural network). Neural networks are modeled after the human brain's structure and function. A neural network consists of interconnected layers of nodes (analogous to neurons) that work together to process and analyze complex data. Neural networks are well suited to tasks that involve identifying complex patterns and relationships in large amounts of data.
The simplest form of machine learning is called supervised learning, which involves the use of labeled data sets to train algorithms to classify data or predict outcomes accurately. In supervised learning, humans pair each training example with an output label. The goal is for the model to learn the mapping between inputs and outputs in the training data, so it can predict the labels of new, unseen data.
Deep learning is a subset of machine learning that uses multilayered neural networks, called deep neural networks, that more closely simulate the complex decision-making power of the human brain.
Deep neural networks include an input layer, at least three but usually hundreds of hidden layers, and an output layer, unlike neural networks used in classic machine learning models, which usually have only one or two hidden layers.
These multiple layers enable unsupervised learning: they can automate the extraction of features from large, unlabeled and unstructured data sets, and make their own predictions about what the data represents.
Because deep learning doesn’t require human intervention, it enables machine learning at a tremendous scale. It is well suited to natural language processing (NLP), computer vision, and other tasks that involve the fast, accurate identification complex patterns and relationships in large amounts of data. Some form of deep learning powers most of the artificial intelligence (AI) applications in our lives today.
Deep learning also enables:
Generative AI, sometimes called "gen AI", refers to deep learning models that can create complex original content such as long-form text, high-quality images, realistic video or audio and more in response to a user’s prompt or request.
At a high level, generative models encode a simplified representation of their training data, and then draw from that representation to create new work that’s similar, but not identical, to the original data.
Generative models have been used for years in statistics to analyze numerical data. But over the last decade, they evolved to analyze and generate more complex data types. This evolution coincided with the emergence of three sophisticated deep learning model types:
Decoding AI: Weekly News Roundup
Join our world-class panel of engineers, researchers, product leaders and more as they cut through the AI noise to bring you the latest in AI news and insights.
How generative AI works
In general, generative AI operates in three phases:
Generative AI begins with a "foundation model"; a deep learning model that serves as the basis for multiple different types of generative AI applications.
The most common foundation models today are large language models (LLMs), created for text generation applications. But there are also foundation models for image, video, sound or music generation, and multimodal foundation models that support several kinds of content.
To create a foundation model, practitioners train a deep learning algorithm on huge volumes of relevant raw, unstructured, unlabeled data, such as terabytes or petabytes of data text or images or video from the internet. The training yields a neural network of billions of parameters encoded representations of the entities, patterns and relationships in the data that can generate content autonomously in response to prompts. This is the foundation model.
This training process is compute-intensive, time-consuming and expensive. It requires thousands of clustered graphics processing units (GPUs) and weeks of processing, all of which typically costs millions of dollars. Open source foundation model projects, such as Meta's Llama-2, enable gen AI developers to avoid this step and its costs.
Next, the model must be tuned to a specific content generation task. This can be done in various ways, including:
Generation, evaluation and more tuning
Developers and users regularly assess the outputs of their generative AI apps, and further tune the model even as often as once a week for greater accuracy or relevance. In contrast, the foundation model itself is updated much less frequently, perhaps every year or 18 months.
Another option for improving a gen AI app's performance is retrieval augmented generation (RAG), a technique for extending the foundation model to use relevant sources outside of the training data to refine the parameters for greater accuracy or relevance.
AI agents and agentic AI
An AI agent is an autonomous AI program, it can perform tasks and accomplish goals on behalf of a user or another system without human intervention, by designing its own workflow and using available tools (other applications or services).
Agentic AI is a system of multiple AI agents, the efforts of which are coordinated, or orchestrated, to accomplish a more complex task or a greater goal than any single agent in the system could accomplish.
Unlike chatbots and other AI models which operate within predefined constraints and require human intervention, AI agents and agentic AI exhibit autonomy, goal-driven behavior and adaptability to changing circumstances. The terms “agent” and “agentic” refer to these models’ agency, or their capacity to act independently and purposefully.
One way to think of agents is as a natural next step after generative AI. Gen AI models focus on creating content based on learned patterns; agents use that content to interact with each other and other tools to make decisions, solve problems and complete tasks. For example, a gen AI app might be able to tell you the best time to climb Mt. Everest given your work schedule, but an agent can tell you this, and then use an online travel service to book you the best flight and reserve a room in the most convenient hotel in Nepal.
AI offers numerous benefits across various industries and applications. Some of the most commonly cited benefits include:
Automation of repetitive tasks
AI can automate routine, repetitive and often tedious tasks including digital tasks such as data collection, entering and preprocessing, and physical tasks such as warehouse stock-picking and manufacturing processes. This automation frees to work on higher value, more creative work.
Enhanced decision-making
Whether used for decision support or for fully automated decision-making, AI enables faster, more accurate predictions and reliable, data-driven decisions. Combined with automation, AI enables businesses to act on opportunities and respond to crises as they emerge, in real time and without human intervention.
AI can reduce human errors in various ways, from guiding people through the proper steps of a process, to flagging potential errors before they occur, and fully automating processes without human intervention. This is especially important in industries such as healthcare where, for example, AI-guided surgical robotics enable consistent precision.
Machine learning algorithms can continually improve their accuracy and further reduce errors as they're exposed to more data and "learn" from experience.
Round-the-clock availability and consistency
AI is always on, available around the clock, and delivers consistent performance every time. Tools such as AI chatbots or virtual assistants can lighten staffing demands for customer service or support. In other applications such as materials processing or production lines, AI can help maintain consistent work quality and output levels when used to complete repetitive or tedious tasks.
Reduced physical risk
By automating dangerous work such as animal control, handling explosives, performing tasks in deep ocean water, high altitudes or in outer space, AI can eliminate the need to put human workers at risk of injury or worse. While they have yet to be perfected, self-driving cars and other vehicles offer the potential to reduce the risk of injury to passengers.
The real-world applications of AI are many. Here is just a small sampling of use cases across various industries to illustrate its potential:
Customer experience, service and support
Companies can implement AI-powered chatbots and virtual assistants to handle customer inquiries, support tickets and more. These tools use natural language processing (NLP) and generative AI capabilities to understand and respond to customer questions about order status, product details and return policies.
Chatbots and virtual assistants enable always-on support, provide faster answers to frequently asked questions (FAQs), free human agents to focus on higher-level tasks, and give customers faster, more consistent service.
Machine learning and deep learning algorithms can analyze transaction patterns and flag anomalies, such as unusual spending or login locations, that indicate fraudulent transactions. This enables organizations to respond more quickly to potential fraud and limit its impact, giving themselves and customers greater peace of mind.
Personalized marketing
Retailers, banks and other customer-facing companies can use AI to create personalized customer experiences and marketing campaigns that delight customers, improve sales and prevent churn. Based on data from customer purchase history and behaviors, deep learning algorithms can recommend products and services customers are likely to want, and even generate personalized copy and special offers for individual customers in real time.
Human resources and recruitment
AI-driven recruitment platforms can streamline hiring by screening resumes, matching candidates with job descriptions, and even conducting preliminary interviews using video analysis. These and other tools can dramatically reduce the mountain of administrative paperwork associated with fielding a large volume of candidates. It can also reduce response times and time-to-hire, improving the experience for candidates whether they get the job or not.
Application development and modernization
Generative AI code generation tools and automation tools can streamline repetitive coding tasks associated with application development, and accelerate the migration and modernization (reformatting and replatorming) of legacy applications at scale. These tools can speed up tasks, help ensure code consistency and reduce errors.
Predictive maintenance
Machine learning models can analyze data from sensors, Internet of Things (IoT) devices and operational technology (OT) to forecast when maintenance will be required and predict equipment failures before they occur. AI-powered preventive maintenance helps prevent downtime and enables you to stay ahead of supply chain issues before they affect the bottom line.
AI challenges and risks
Organizations are scrambling to take advantage of the latest AI technologies and capitalize on AI's many benefits. This rapid adoption is necessary, but adopting and maintaining AI workflows comes with challenges and risks.
AI systems rely on data sets that might be vulnerable to data poisoning, data tampering, data bias or cyberattacks that can lead to data breaches. Organizations can mitigate these risks by protecting data integrity and implementing security and availability throughout the entire AI lifecycle, from development to training and deployment and postdeployment.
Threat actors can target AI models for theft, reverse engineering or unauthorized manipulation. Attackers might compromise a model’s integrity by tampering with its architecture, weights or parameters; the core components that determine a model’s behavior, accuracy and performance.
Like all technologies, models are susceptible to operational risks such as model drift, bias and breakdowns in the governance structure. Left unaddressed, these risks can lead to system failures and cybersecurity vulnerabilities that threat actors can use.
Ethics and legal risks
If organizations don’t prioritize safety and ethics when developing and deploying AI systems, they risk committing privacy violations and producing biased outcomes. For example, biased training data used for hiring decisions might reinforce gender or racial stereotypes and create AI models that favor certain demographic groups over others.
AI ethics and governance
AI ethics is a multidisciplinary field that studies how to optimize AI's beneficial impact while reducing risks and adverse outcomes. Principles of AI ethics are applied through a system of AI governance consisted of guardrails that help ensure that AI tools and systems remain safe and ethical.
AI governance encompasses oversight mechanisms that address risks. An ethical approach to AI governance requires the involvement of a wide range of stakeholders, including developers, users, policymakers and ethicists, helping to ensure that AI-related systems are developed and used to align with society's values.
Here are common values associated with AI ethics and responsible AI:
As AI becomes more advanced, humans are challenged to comprehend and retrace how the algorithm came to a result. Explainable AI is a set of processes and methods that enables human users to interpret, comprehend and trust the results and output created by algorithms.
Although machine learning, by its very nature, is a form of statistical discrimination, the discrimination becomes objectionable when it places privileged groups at systematic advantage and certain unprivileged groups at systematic disadvantage, potentially causing varied harms. To encourage fairness, practitioners can try to minimize algorithmic bias across data collection and model design, and to build more diverse and inclusive teams.
Robust AI effectively handles exceptional conditions, such as abnormalities in input or malicious attacks, without causing unintentional harm. It is also built to withstand intentional and unintentional interference by protecting against exposed vulnerabilities.
Organizations should implement clear responsibilities and governance
structures for the development, deployment and outcomes of AI systems.
In addition, users should be able to see how an AI service works,
evaluate its functionality, and comprehend its strengths and
limitations. Increased transparency provides information for AI
consumers to better understand how the AI model or service was created.
Many regulatory frameworks, including GDPR, mandate that organizations abide by certain privacy principles when processing personal information. It is crucial to be able to protect AI models that might contain personal information, control what data goes into the model in the first place, and to build adaptable systems that can adjust to changes in regulation and attitudes around AI ethics.
Weak AI vs. Strong AI
In order to contextualize the use of AI at various levels of complexity and sophistication, researchers have defined several types of AI that refer to its level of sophistication:
Weak AI: Also known as “narrow AI,” defines AI systems designed to perform a specific task or a set of tasks. Examples might include “smart” voice assistant apps, such as Amazon’s Alexa, Apple’s Siri, a social media chatbot or the autonomous vehicles promised by Tesla.
Strong AI: Also known as “artificial general intelligence” (AGI) or “general AI,” possess the ability to understand, learn and apply knowledge across a wide range of tasks at a level equal to or surpassing human intelligence. This level of AI is currently theoretical and no known AI systems approach this level of sophistication. Researchers argue that if AGI is even possible, it requires major increases in computing power. Despite recent advances in AI development, self-aware AI systems of science fiction remain firmly in that realm.
The idea of "a machine that thinks" dates back to ancient Greece. But since the advent of electronic computing (and relative to some of the topics discussed in this article) important events and milestones in the evolution of AI include the following:
Alan Turing publishes Computing Machinery and Intelligence. In this paper, Turing famous for breaking the German ENIGMA code during WWII and often referred to as the "father of computer science" asks the following question: "Can machines think?"
From there, he offers a test, now famously known as the "Turing Test," where a human interrogator would try to distinguish between a computer and human text response. While this test has undergone much scrutiny since it was published, it remains an important part of the history of AI, and an ongoing concept within philosophy as it uses ideas around linguistics.
John McCarthy coins the term "artificial intelligence" at the first-ever AI conference at Dartmouth College. (McCarthy went on to invent the Lisp language.) Later that year, Allen Newell, J.C. Shaw and Herbert Simon create the Logic Theorist, the first-ever running AI computer program.
Frank Rosenblatt builds the Mark 1 Perceptron, the first computer based on a neural network that "learned" through trial and error. Just a year later, Marvin Minsky and Seymour Papert publish a book titled Perceptrons, which becomes both the landmark work on neural networks and, at least for a while, an argument against future neural network research initiatives.
Neural networks, which use a backpropagation algorithm to train itself, became widely used in AI applications.
Stuart Russell and Peter Norvig publish Artificial Intelligence: A Modern Approach, which becomes one of the leading textbooks in the study of AI. In it, they delve into four potential goals or definitions of AI, which differentiates computer systems based on rationality and thinking versus acting.
IBM's Deep Blue beats then world chess champion Garry Kasparov, in a chess match (and rematch).
John McCarthy writes a paper, What Is Artificial Intelligence?, and proposes an often-cited definition of AI. By this time, the era of big data and cloud computing is underway, enabling organizations to manage ever-larger data estates, which will one day be used to train AI models.
IBM Watson® beats champions Ken Jennings and Brad Rutter at Jeopardy! Also, around this time, data science begins to emerge as a popular discipline.
Baidu's Minwa supercomputer uses a special deep neural network called a convolutional neural network to identify and categorize images with a higher rate of accuracy than the average human.
DeepMind's AlphaGo program, powered by a deep neural network, beats Lee Sodol, the world champion Go player, in a five-game match. The victory is significant given the huge number of possible moves as the game progresses (over 14.5 trillion after just four moves). Later, Google purchased DeepMind for a reported USD 400 million.
A rise in large language models or LLMs, such as OpenAI’s ChatGPT, creates an enormous change in performance of AI and its potential to drive enterprise value. With these new generative AI practices, deep-learning models can be pretrained on large amounts of data.
The latest AI trends point to a continuing AI renaissance. Multimodal models that can take multiple types of data as input are providing richer, more robust experiences. These models bring together computer vision image recognition and NLP speech recognition capabilities. Smaller models are also making strides in an age of diminishing returns with massive models with large parameter counts.
Learn how to choose the right approach in preparing datasets and employing foundation models.
Learn how to choose the right approach in preparing datasets and employing foundation models.
Learn why IBM has been recognized as a Leader in the 2025 Gartner® Magic Quadrant™ for Data Science and Machine Learning Platforms.
Learn why IBM has been recognized as a Leader in the 2025 Gartner® Magic Quadrant™ for Data Science and Machine Learning Platforms.
Learn how organizations are shifting from launching AI in disparate pilots to using it to drive transformation at the core.
Learn how organizations are shifting from launching AI in disparate pilots to using it to drive transformation at the core.
Access our full catalog of over 100 online courses by purchasing an individual or multi-user subscription today, enabling you to expand your skills across a range of our products at a low price.
Access our full catalog of over 100 online courses by purchasing an individual or multi-user subscription today, enabling you to expand your skills across a range of our products at a low price.
IBM® Granite® is a family of open, performant and trusted AI models tailored for business and optimized to scale your AI applications. Explore language, code, time series and guardrail options.
IBM® Granite® is a family of open, performant and trusted AI models tailored for business and optimized to scale your AI applications. Explore language, code, time series and guardrail options.
Led by top IBM thought leaders, the curriculum is designed to help business leaders gain the knowledge needed to prioritize the AI investments that can drive growth.
Led by top IBM thought leaders, the curriculum is designed to help business leaders gain the knowledge needed to prioritize the AI investments that can drive growth.
We surveyed 2,000 organizations about their AI initiatives to discover what’s working, what’s not and how you can get ahead.
We surveyed 2,000 organizations about their AI initiatives to discover what’s working, what’s not and how you can get ahead.
Activate these five mindshifts to cut through the uncertainty, spur business reinvention, and supercharge growth with agentic AI.
Activate these five mindshifts to cut through the uncertainty, spur business reinvention, and supercharge growth with agentic AI.
Learn how to confidently incorporate generative AI and machine learning into your business.
Learn how to confidently incorporate generative AI and machine learning into your business.
Dive into the three critical elements of a strong AI strategy: creating a competitive edge, scaling AI across the business and advancing trustworthy AI.
Dive into the three critical elements of a strong AI strategy: creating a competitive edge, scaling AI across the business and advancing trustworthy AI.
Train, validate, tune and deploy generative AI, foundation models and machine learning capabilities with IBM watsonx.ai, a next-generation enterprise studio for AI builders. Build AI applications in a fraction of the time with a fraction of the data.
Put AI to work in your business with IBM’s industry-leading AI expertise and portfolio of solutions at your side.
IBM Consulting AI services help reimagine how businesses work with AI for transformation.
Get one-stop access to capabilities that span the AI development lifecycle. Produce powerful AI solutions with user-friendly interfaces, workflows and access to industry-standard APIs and SDKs.
Powered by IBM watsonx
Powered by IBM watsonx
- Account informationCurrentWe use your email to validate you are who you say you are, to create your IBMid, and to contact you for account related matters.Business emailYour subscription will be delivered in English. You will find an unsubscribe link in every newsletter. You can manage your subscriptions or unsubscribe  here. Refer to our  IBM Privacy Statement for more information.
- Account informationCurrent
- Semi-supervised learning, which combines supervised and unsupervised learning by using both labeled and unlabeled data to train AI models for classification and regression tasks.
- Self-supervised learning, which generates implicit labels from unstructured data, rather than relying on labeled data sets for supervisory signals.
- Reinforcement learning, which learns by trial-and-error and reward functions rather than by extracting information from hidden patterns.
- Transfer learning, in which knowledge gained through one task or data set is used to improve model performance on another related task or different data set.
- Variational autoencoders or VAEs, which were introduced in 2013, and enabled models that could generate multiple variations of content in response to a prompt or instruction.
- Diffusion models, first seen in 2014, which add "noise" to images until they are unrecognizable, and then remove the noise to generate original images in response to prompts.
- Transformers (also called transformer models), which are trained on sequenced data to generate extended sequences of content (such as words in sentences, shapes in an image, frames of a video or commands in software code). Transformers are at the core of most of today’s headline-making generative AI tools, including ChatGPT and GPT-4, Copilot, BERT, Bard and Midjourney.
- Training, to create a foundation model.
- Tuning, to adapt the model to a specific application.
- Generation, evaluation and more tuning, to improve accuracy.
- Fine-tuning, which involves feeding the model application-specific labeled data, questions or prompts the application is likely to receive, and corresponding correct answers in the wanted format.
- Reinforcement learning with human feedback (RLHF), in which human users evaluate the accuracy or relevance of model outputs so that the model can improve itself. This can be as simple as having people type or talk back corrections to a chatbot or virtual assistant.
- Automation of repetitive tasks.
- More and faster insight from data.
- Enhanced decision-making.
- Fewer human errors.
- 24x7 availability.
- Reduced physical risks.

--------------------------------------------------

URL: https://www.ibm.com/think/topics/machine-learning
==================================================
What is machine learning?
What is machine learning?
Machine learning (ML) is a branch of artificial intelligence (AI) focused on enabling computers and machines to imitate the way that humans learn, to perform tasks autonomously, and to improve their performance and accuracy through experience and exposure to more data.
UC Berkeley breaks out the learning system of a machine learning algorithm into three main parts.
The latest AI trends, brought to you by experts
Get curated insights on the most important—and intriguing—AI news. Subscribe to our weekly Think newsletter. See the IBM Privacy Statement.
Thank you! You are subscribed.
Your subscription will be delivered in English. You will find an unsubscribe link in every newsletter. You can manage your subscriptions or unsubscribe  here. Refer to our  IBM Privacy Statement for more information.
Your subscription will be delivered in English. You will find an unsubscribe link in every newsletter. You can manage your subscriptions or unsubscribe here. Refer to our IBM Privacy Statement for more information.
Machine learning versus deep learning versus neural networks
Since deep learning and machine learning tend to be used interchangeably, it’s worth noting the nuances between the two. Machine learning, deep learning, and neural networks are all sub-fields of artificial intelligence. However, neural networks is actually a sub-field of machine learning, and deep learning is a sub-field of neural networks.
The way in which deep learning and machine learning differ is in how each algorithm learns. "Deep" machine learning can use labeled datasets, also known as supervised learning, to inform its algorithm, but it doesn’t necessarily require a labeled dataset. The deep learning process can ingest unstructured data in its raw form (e.g., text or images), and it can automatically determine the set of features which distinguish different categories of data from one another. This eliminates some of the human intervention required and enables the use of large amounts of data. You can think of deep learning as "scalable machine learning" as Lex Fridman notes in this MIT lecture1.
Classical, or "non-deep," machine learning is more dependent on human intervention to learn. Human experts determine the set of features to understand the differences between data inputs, usually requiring more structured data to learn.
Neural networks, or artificial neural networks (ANNs), are comprised of node layers, containing an input layer, one or more hidden layers, and an output layer. Each node, or artificial neuron, connects to another and has an associated weight and threshold. If the output of any individual node is above the specified threshold value, that node is activated, sending data to the next layer of the network. Otherwise, no data is passed along to the next layer of the network by that node. The “deep” in deep learning is just referring to the number of layers in a neural network. A neural network that consists of more than three layers, which would be inclusive of the input and the output can be considered a deep learning algorithm or a deep neural network. A neural network that only has three layers is just a basic neural network.
Deep learning and neural networks are credited with accelerating progress in areas such as computer vision, natural language processing (NLP), and speech recognition.
See the blog post “AI vs. Machine Learning vs. Deep Learning vs. Neural Networks: What’s the Difference?” for a closer look at how the different concepts relate.
Decoding AI: Weekly News Roundup
Join our world-class panel of engineers, researchers, product leaders and more as they cut through the AI noise to bring you the latest in AI news and insights.
Machine learning methods
Machine learning models fall into three primary categories.
Supervised learning, also known as supervised machine learning, is defined by its use of labeled datasets to train algorithms to classify data or predict outcomes accurately. As input data is fed into the model, the model adjusts its weights until it has been fitted appropriately. This occurs as part of the cross validation process to ensure that the model avoids overfitting or underfitting. Supervised learning helps organizations solve a variety of real-world problems at scale, such as classifying spam in a separate folder from your inbox. Some methods used in supervised learning include neural networks, Naïve Bayes, linear regression, logistic regression, random forest, and support vector machine (SVM).
Unsupervised learning
Unsupervised learning, also known as unsupervised machine learning, uses machine learning algorithms to analyze and cluster unlabeled datasets (subsets called clusters). These algorithms discover hidden patterns or data groupings without the need for human intervention.
Unsupervised learning’s ability to discover similarities and differences in information make it ideal for exploratory data analysis, cross-selling strategies, customer segmentation, and image and pattern recognition. It’s also used to reduce the number of features in a model through the process of dimensionality reduction. Principal component analysis (PCA) and singular value decomposition (SVD) are two common approaches for this. Other algorithms used in unsupervised learning include neural networks, k-means clustering, and probabilistic clustering methods.
Semi-supervised learning
Semi-supervised learning offers a happy medium between supervised and unsupervised learning. During training, it uses a smaller labeled data set to guide classification and feature extraction from a larger, unlabeled data set. Semi-supervised learning can solve the problem of not having enough labeled data for a supervised learning algorithm. It also helps if it’s too costly to label enough data.
For a deep dive into the differences between these approaches, check out "Supervised vs. Unsupervised Learning: What's the Difference?"
Reinforcement learning
Reinforcement learning is a machine learning model that is similar to supervised learning, but the algorithm isn’t trained using sample data. This model learns as it goes by using trial and error. A sequence of successful outcomes will be reinforced to develop the best recommendation or policy for a given problem.
The IBM Watson® system that won the Jeopardy! challenge in 2011 is a good example. The system used reinforcement learning to learn when to attempt an answer (or question, as it were), which square to select on the board, and how much to wager, especially on daily doubles.
Common machine learning algorithms
A number of machine learning algorithms are commonly used. These include:
Neural networks simulate the way the human brain works, with a huge number of linked processing nodes. Neural networks are good at recognizing patterns and play an important role in applications including natural language translation, image recognition, speech recognition, and image creation.
This algorithm is used to predict numerical values, based on a linear relationship between different values. For example, the technique could be used to predict house prices based on historical data for the area.
This supervised learning algorithm makes predictions for categorical response variables, such as “yes/no” answers to questions. It can be used for applications such as classifying spam and quality control on a production line.
Using unsupervised learning, clustering algorithms can identify patterns in data so that it can be grouped. Computers can help data scientists by identifying differences between data items that humans have overlooked.
Decision trees can be used for both predicting numerical values (regression) and classifying data into categories. Decision trees use a branching sequence of linked decisions that can be represented with a tree diagram. One of the advantages of decision trees is that they are easy to validate and audit, unlike the black box of the neural network.
In a random forest, the machine learning algorithm predicts a value or category by combining the results from a number of decision trees.
Advantages and disadvantages of machine learning algorithms
Depending on your budget, need for speed and precision required, each algorithm type supervised, unsupervised, semi-supervised, or reinforcement has its own advantages and disadvantages.
For example, decision tree algorithms are used for both predicting numerical values (regression problems) and classifying data into categories. Decision trees use a branching sequence of linked decisions that may be represented with a tree diagram. A prime advantage of decision trees is that they are easier to validate and audit than a neural network. The bad news is that they can be more unstable than other decision predictors.
Overall, there are many advantages to machine learning that businesses can leverage for new efficiencies. These include machine learning identifying patterns and trends in massive volumes of data that humans might not spot at all. And this analysis requires little human intervention: just feed in the dataset of interest and let the machine learning system assemble and refine its own algorithms, which will continually improve with more data input over time. Customers and users can enjoy a more personalized experience as the model learns more with every experience with that person.
On the downside, machine learning requires large training datasets that are accurate and unbiased. GIGO is the operative factor: garbage in / garbage out. Gathering sufficient data and having a system robust enough to run it might also be a drain on resources.
Machine learning can also be prone to error, depending on the input. With too small a sample, the system could produce a perfectly logical algorithm that is completely wrong or misleading. To avoid wasting budget or displeasing customers, organizations should act on the answers only when there is high confidence in the output.
Real-world machine learning use cases
Here are just a few examples of machine learning you might encounter every day:
Generative AI: Generative AI, or gen AI, is machine learning that can create original content—text, images, video, software code—in response to a user’s prompt or request. Gen AI relies on deep learning models that identify and encode the patterns and relationships in huge amounts of data, and then use that information to understand users’ requests and create new content. ChatGPT and Claude.ai are examples of generative AI apps.
AI agents and agentic AI: An AI agent is an autonomous AI program—it can perform tasks and accomplish goals on behalf of a user or another system without human intervention, by designing its own workflow and using available tools (other applications or services). Agentic AI is a system of multiple AI agents, the efforts of which are coordinated, or orchestrated, to accomplish a more complex task or a greater goal than any single agent in the system could accomplish.
Speech recognition: Also known as automatic speech recognition (ASR), computer speech recognition, or speech-to-text, speech recognition uses natural language processing (NLP) to translate human speech into a written format. Many mobile devices incorporate speech recognition into their systems to conduct voice search e.g. Siri or improve accessibility for texting.
Customer service: Online chatbots are replacing human agents along the customer journey, changing the way we think about customer engagement across websites and social media platforms. Chatbots answer frequently asked questions (FAQs) about topics such as shipping, or provide personalized advice, cross-selling products or suggesting sizes for users. Examples include virtual agents on e-commerce sites; messaging bots, using Slack and Facebook Messenger; and tasks usually done by virtual assistants and voice assistants.
Computer vision: This AI technology enables computers to derive meaningful information from digital images, videos, and other visual inputs, and then take the appropriate action. Powered by convolutional neural networks, computer vision has applications in photo tagging on social media, radiology imaging in healthcare, and self-driving cars in the automotive industry.
Recommendation engines: Using past consumption behavior data, AI algorithms can help to discover data trends that can be used to develop more effective cross-selling strategies. Recommendation engines are used by online retailers to make relevant product recommendations to customers during the checkout process.
Robotic process automation (RPA): Also known as software robotics, RPA uses intelligent automation technologies to perform repetitive manual tasks.
Automated stock trading: Designed to optimize stock portfolios, AI-driven high-frequency trading platforms make thousands or even millions of trades per day without human intervention.
Fraud detection: Banks and other financial institutions can use machine learning to spot suspicious transactions. Supervised learning can train a model using information about known fraudulent transactions. Anomaly detection can identify transactions that look atypical and deserve further investigation.
Challenges of machine learning
As machine learning technology has developed, it has certainly made our lives easier. However, implementing machine learning in businesses has also raised a number of ethical concerns about AI technologies. Some of these include:
Technological singularity
While this topic garners a lot of public attention, many researchers are not concerned with the idea of AI surpassing human intelligence in the near future. Technological singularity is also referred to as strong AI or superintelligence. Philosopher Nick Bostrum defines superintelligence as “any intellect that vastly outperforms the best human brains in practically every field, including scientific creativity, general wisdom, and social skills.”
Despite the fact that superintelligence is not imminent in society, the idea of it raises some interesting questions as we consider the use of autonomous systems, like self-driving cars. It’s unrealistic to think that a driverless car would never have an accident, but who is responsible and liable under those circumstances? Should we still develop autonomous vehicles, or do we limit this technology to semi-autonomous vehicles which help people drive safely? The jury is still out on this, but these are the types of ethical debates that are occurring as new, innovative AI technology develops.
While a lot of public perception of artificial intelligence centers around job losses, this concern should probably be reframed. With every disruptive, new technology, we see that the market demand for specific job roles shifts. For example, when we look at the automotive industry, many manufacturers, like GM, are shifting to focus on electric vehicle production to align with green initiatives. The energy industry isn’t going away, but the source of energy is shifting from a fuel economy to an electric one.
In a similar way, artificial intelligence will shift the demand for jobs to other areas. There will need to be individuals to help manage AI systems. There will still need to be people to address more complex problems within the industries that are most likely to be affected by job demand shifts, such as customer service. The biggest challenge with artificial intelligence and its effect on the job market will be helping people to transition to new roles that are in demand.
Privacy tends to be discussed in the context of data privacy, data protection, and data security. These concerns have allowed policymakers to make more strides in recent years. For example, in 2016, GDPR legislation was created to protect the personal data of people in the European Union and European Economic Area, giving individuals more control of their data. In the United States, individual states are developing policies, such as the California Consumer Privacy Act (CCPA), which was introduced in 2018 and requires businesses to inform consumers about the collection of their data. Legislation such as this has forced companies to rethink how they store and use personally identifiable information (PII). As a result, investments in security have become an increasing priority for businesses as they seek to eliminate any vulnerabilities and opportunities for surveillance, hacking, and cyberattacks.
Bias and discrimination
Instances of bias and discrimination across a number of machine learning systems have raised many ethical questions regarding the use of artificial intelligence. How can we safeguard against bias and discrimination when the training data itself may be generated by biased human processes? While companies typically have good intentions for their automation efforts, Reuters2 highlights some of the unforeseen consequences of incorporating AI into hiring practices. In their effort to automate and simplify a process, Amazon unintentionally discriminated against job candidates by gender for technical roles, and the company ultimately had to scrap the project. Harvard Business Review3 has raised other pointed questions about the use of AI in hiring practices, such as what data you should be able to use when evaluating a candidate for a role.
Bias and discrimination aren’t limited to the human resources function either; they can be found in a number of applications from facial recognition software to social media algorithms. As businesses become more aware of the risks with AI, they’ve also become more active in this discussion around AI ethics and values.
Since there isn’t significant legislation to regulate AI practices, there is no real enforcement mechanism to ensure that ethical AI is practiced. The current incentives for companies to be ethical are the negative repercussions of an unethical AI system on the bottom line. To fill the gap, ethical frameworks have emerged as part of a collaboration between ethicists and researchers to govern the construction and distribution of AI models within society. However, at the moment, these only serve to guide. Some research4 shows that the combination of distributed responsibility and a lack of foresight into potential consequences aren’t conducive to preventing harm to society.
How to choose the right AI platform for machine learning
Selecting a platform can be a challenging process, as the wrong system can drive up costs, or limit the use of other valuable tools or technologies. When reviewing multiple vendors to select an AI platform, there is often a tendency to think that more features = a better system. Maybe so, but reviewers should start by thinking through what the AI platform will be doing for their organization. What machine learning capabilities need to be delivered and what features are important to accomplish them? One missing feature might doom the usefulness of an entire system. Here are some features to consider.
MLOps capabilities. Does the system have:
Generative AI capabilities. Does the system have:
Learn why IBM has been recognized as a Leader in the 2025 Gartner® Magic Quadrant™ for Data Science and Machine Learning Platforms.
Learn why IBM has been recognized as a Leader in the 2025 Gartner® Magic Quadrant™ for Data Science and Machine Learning Platforms.
Activate these five mindshifts to cut through the uncertainty, spur business reinvention, and supercharge growth with agentic AI.
Activate these five mindshifts to cut through the uncertainty, spur business reinvention, and supercharge growth with agentic AI.
Learn fundamental concepts and build your skills with hands-on labs, courses, guided projects, trials and more.
Learn fundamental concepts and build your skills with hands-on labs, courses, guided projects, trials and more.
Learn how to confidently incorporate generative AI and machine learning into your business.
Learn how to confidently incorporate generative AI and machine learning into your business.
Want to get a better return on your AI investments? Learn how scaling gen AI in key areas drives change by helping your best minds build and deliver innovative new solutions.
Want to get a better return on your AI investments? Learn how scaling gen AI in key areas drives change by helping your best minds build and deliver innovative new solutions.
Learn how to select the most suitable AI foundation model for your use case.
Learn how to select the most suitable AI foundation model for your use case.
IBM® Granite™ is our family of open, performant and trusted AI models, tailored for business and optimized to scale your AI applications. Explore language, code, time series and guardrail options.
IBM® Granite™ is our family of open, performant and trusted AI models, tailored for business and optimized to scale your AI applications. Explore language, code, time series and guardrail options.
Dive into the 3 critical elements of a strong AI strategy: creating a competitive edge, scaling AI across the business and advancing trustworthy AI.
Dive into the 3 critical elements of a strong AI strategy: creating a competitive edge, scaling AI across the business and advancing trustworthy AI.
We surveyed 2,000 organizations about their AI initiatives to discover what’s working, what’s not and how you can get ahead.
We surveyed 2,000 organizations about their AI initiatives to discover what’s working, what’s not and how you can get ahead.
Train, validate, tune and deploy generative AI, foundation models and machine learning capabilities with IBM watsonx.ai, a next-generation enterprise studio for AI builders. Build AI applications in a fraction of the time with a fraction of the data.
Move your applications from prototype to production with the help of our AI development solutions.
Reinvent critical workflows and operations by adding AI to maximize experiences, real-time decision-making and business value.
Get one-stop access to capabilities that span the AI development lifecycle. Produce powerful AI solutions with user-friendly interfaces, workflows and access to industry-standard APIs and SDKs.
Powered by IBM watsonx
Powered by IBM watsonx
- A Decision Process: In general, machine learning algorithms are used to make a prediction or classification. Based on some input data, which can be labeled or unlabeled, your algorithm will produce an estimate about a pattern in the data.
- An Error Function: An error function evaluates the prediction of the model. If there are known examples, an error function can make a comparison to assess the accuracy of the model.
- A Model Optimization Process: If the model can fit better to the data points in the training set, then weights are adjusted to reduce the discrepancy between the known example and the model estimate. The algorithm will repeat this iterative “evaluate and optimize” process, updating weights autonomously until a threshold of accuracy has been met.
- Account informationCurrentWe use your email to validate you are who you say you are, to create your IBMid, and to contact you for account related matters.Business emailYour subscription will be delivered in English. You will find an unsubscribe link in every newsletter. You can manage your subscriptions or unsubscribe  here. Refer to our  IBM Privacy Statement for more information.
- Account informationCurrent
- Neural networks
- Linear regression
- Logistic regression
- Decision trees
- Random forests
- a unified interface for ease of management?
- automated machine learning tools for faster model creation with low-code and no-code functionality?
- decision optimization to streamline the selection and deployment of optimization models?
- visual modeling to combine visual data science with open-source libraries and notebook-based interfaces on a unified data and AI studio?
- automated development for beginners to get started quickly and more advanced data scientists to experiment?
- synthetic data generator as an alternative or supplement to real-world data when real-world data is not readily available?
- a content generator that can generate text, images and other content based on the data it was trained on?
- automated classification to read and classify written input, such as evaluating and sorting customer complaints or reviewing customer feedback sentiment?
- a summary generator that can transform dense text into a high-quality summary, capture key points from financial reports, and generate meeting transcriptions?
- a data extraction capability to sort through complex details and quickly pull the necessary information from large documents?

--------------------------------------------------

URL: https://www.ibm.com/think/topics/deep-learning
==================================================
What is deep learning?
Editor, Topics & Insights for IBM Think
What is deep learning?
Deep learning is a subset of machine learning that uses multilayered neural networks, called deep neural networks, to simulate the complex decision-making power of the human brain. Some form of deep learning powers most of the artificial intelligence (AI) applications in our lives today.
The chief difference between deep learning and machine learning is the structure of the underlying neural network architecture. “Nondeep,” traditional machine learning models use simple neural networks with one or two computational layers. Deep learning models use three or more layers, but typically hundreds or thousands of layers to train the models.
While supervised learning models require structured, labeled input data to make accurate outputs, deep learning models can use unsupervised learning. With unsupervised learning, deep learning models can extract the characteristics, features and relationships they need to make accurate outputs from raw, unstructured data. Additionally, these models can even evaluate and refine their outputs for increased precision.
Deep learning is an aspect of data science that drives many applications and services that improve automation, performing analytical and physical tasks without human intervention. This enables many everyday products and services, such as digital assistants, voice-enabled TV remotes, credit card fraud detection, self-driving cars and generative AI.
The latest AI trends, brought to you by experts
Get curated insights on the most important—and intriguing—AI news. Subscribe to our weekly Think newsletter. See the IBM Privacy Statement.
Thank you! You are subscribed.
Your subscription will be delivered in English. You will find an unsubscribe link in every newsletter. You can manage your subscriptions or unsubscribe  here. Refer to our  IBM Privacy Statement for more information.
Your subscription will be delivered in English. You will find an unsubscribe link in every newsletter. You can manage your subscriptions or unsubscribe here. Refer to our IBM Privacy Statement for more information.
How deep learning works
Neural networks, or artificial neural networks, attempt to mimic the human brain through a combination of data inputs, weights and bias, all acting as silicon neurons. These elements work together to accurately recognize, classify and describe objects within the data.
Deep neural networks consist of multiple layers of interconnected nodes, each building on the previous layer to refine and optimize the prediction or categorization. This progression of computations through the network is called forward propagation. The input and output layers of a deep neural network are called visible layers. The input layer is where the deep learning model ingests the data for processing, and the output layer is where the final prediction or classification is made.
Another process called backpropagation uses algorithms, such as gradient descent, to calculate errors in predictions, and then adjusts the weights and biases of the function by moving backwards through the layers to train the model. Together, forward propagation and backpropagation enable a neural network to make predictions and correct for any errors. Over time, the algorithm becomes gradually more accurate.
Deep learning requires a tremendous amount of computing power. High-performance graphical processing units (GPUs) are ideal because they can handle a large volume of calculations in multiple cores with copious memory available. Distributed cloud computing might also assist. This level of computing power is necessary to train deep algorithms through deep learning. However, managing multiple GPUs on premises can create a large demand on internal resources and be incredibly costly to scale. For software requirements, most deep learning apps are coded with one of these three learning frameworks: JAX, PyTorch or TensorFlow.
Decoding AI: Weekly News Roundup
Join our world-class panel of engineers, researchers, product leaders and more as they cut through the AI noise to bring you the latest in AI news and insights.
Types of deep learning models
Deep learning algorithms are incredibly complex, and there are different types of neural networks to address specific problems or datasets. Here are six. Each has its own advantages and they are presented here roughly in the order of their development, with each successive model adjusting to overcome a weakness in a previous model.
One potential weakness across them all is that deep learning models are often “black boxes,” making it difficult to understand their inner workings and posing interpretability challenges. But this can be balanced against the overall benefits of high accuracy and scalability.
Convolutional neural networks (CNNs or ConvNets) are used primarily in computer vision and image classification applications. They can detect features and patterns within images and videos, enabling tasks such as object detection, image recognition, pattern recognition and face recognition. These networks harness principles from linear algebra, particularly matrix multiplication, to identify patterns within an image.
CNNs are a specific type of neural network, which is composed of node layers, containing an input layer, one or more hidden layers and an output layer. Each node connects to another and has an associated weight and threshold. If the output of any individual node is above the specified threshold value, that node is activated, sending data to the next layer of the network. Otherwise, no data is passed along to the next layer of the network.
At least three main types of layers make up a CNN: a convolutional layer, pooling layer and fully connected (FC) layer. For complex uses, a CNN might contain up to thousands of layers, each layer building on the previous layers. By “convolution” working and reworking the original input detailed patterns can be discovered. With each layer, the CNN increases in its complexity, identifying greater portions of the image. Earlier layers focus on simple features, such as colors and edges. As the image data progresses through the layers of the CNN, it starts to recognize larger elements or shapes of the object until it finally identifies the intended object.
CNNs are distinguished from other neural networks by their superior performance with image, speech or audio signal inputs. Before CNNs, manual and time-consuming feature extraction methods were used to identify objects in images. However, CNNs now provide a more scalable approach to image classification and object recognition tasks, and process high-dimensional data. And CNNs can exchange data between layers, to deliver more efficient data processing. While information might be lost in the pooling layer, this might be outweighed by the benefits of CNNs, which can help to reduce complexity, improve efficiency and limit risk of overfitting.
There are other disadvantages to CNNs, which are computationally demanding costing time and budget, requiring many graphical processing units (GPUs). They also require highly trained experts with cross-domain knowledge, and careful testing of configurations, hyperparameters and configurations.
Recurrent neural networks (RNNs) are typically used in natural language and speech recognition applications as they use sequential or time-series data. RNNs can be identified by their feedback loops. These learning algorithms are primarily used when using time-series data to make predictions about future outcomes. Use cases include stock market predictions or sales forecasting, or ordinal or temporal problems, such as language translation, natural language processing (NLP), speech recognition and image captioning. These functions are often incorporated into popular applications such as Siri, voice search and Google Translate.
RNNs use their “memory” as they take information from prior inputs to influence the current input and output. While traditional deep neural networks assume that inputs and outputs are independent of each other, the output of RNNs depends on the prior elements within the sequence. While future events would also be helpful in determining the output of a given sequence, unidirectional recurrent neural networks cannot account for these events in their predictions.
RNNs share parameters across each layer of the network and share the same weight parameter within each layer of the network, with the weights adjusted through the processes of backpropagation and gradient descent to facilitate reinforcement learning.
RNNs use a backpropagation through time (BPTT) algorithm to determine the gradients, which is slightly different from traditional backpropagation as it is specific to sequence data. The principles of BPTT are the same as traditional backpropagation, where the model trains itself by calculating errors from its output layer to its input layer. BPTT differs from the traditional approach in that BPTT sums errors at each time step, whereas feedforward networks do not need to sum errors as they do not share parameters across each layer.
An advantage over other neural network types is that RNNs use both binary data processing and memory. RNNs can plan out multiple inputs and productions so that rather than delivering only one result for a single input, RNNs can produce one-to-many, many-to-one or many-to-many outputs.
There are also options within RNNs. For example, the long short-term memory (LSTM) network is superior to simple RNNs by learning and acting on longer-term dependencies.
However, RNNs tend to run into two basic problems, known as exploding gradients and vanishing gradients. These issues are defined by the size of the gradient, which is the slope of the loss function along the error curve.
Some final disadvantages: RNNs might also require long training time and be difficult to use on large datasets. Optimizing RNNs add complexity when they have many layers and parameters.
Autoencoders and variational autoencoders
Deep learning made it possible to move beyond the analysis of numerical data, by adding the analysis of images, speech and other complex data types. Among the first class of models to achieve this were variational autoencoders (VAEs). They were the first deep-learning models to be widely used for generating realistic images and speech, which empowered deep generative modeling by making models easier to scale, which is the cornerstone of what we think of as generative AI.
Autoencoders work by encoding unlabeled data into a compressed representation, and then decoding the data back into its original form. Plain autoencoders were used for a variety of purposes, including reconstructing corrupted or blurry images. Variational autoencoders added the critical ability not just to reconstruct data, but also to output variations on the original data.
This ability to generate novel data ignited a rapid-fire succession of new technologies, from generative adversarial networks (GANs) to diffusion models, capable of producing ever more realistic but fake images. In this way, VAEs set the stage for today’s generative AI.
Autoencoders are built out of blocks of encoders and decoders, an architecture that also underpins today’s large language models. Encoders compress a dataset into a dense representation, arranging similar data points closer together in an abstract space. Decoders sample from this space to create something new while preserving the dataset’s most important features.
The biggest advantage to autoencoders is the ability to handle large batches of data and show input data in a compressed form, so the most significant aspects stand out, enabling anomaly detection and classification tasks. This also speeds transmission and reduces storage requirements. Autoencoders can be trained on unlabeled data so they might be used where labeled data is not available. When unsupervised training is used, there is a time savings advantage: deep learning algorithms learn automatically and gain accuracy without needing manual feature engineering. In addition, VAEs can generate new sample data for text or image generation.
There are disadvantages to autoencoders. The training of deep or intricate structures can be a drain on computational resources. And during unsupervised training, the model might overlook the needed properties and instead simply replicate the input data. Autoencoders might also overlook complex data linkages in structured data so that it does not correctly identify complex relationships.
Generative adversarial networks (GANs) are neural networks that are used both in and outside of artificial intelligence (AI) to create new data resembling the original training data. These can include images appearing to be human faces but are generated, not taken of real people. The “adversarial” part of the name comes from the back-and-forth between the two portions of the GAN: a generator and a discriminator.
GANs train themselves. The generator creates fakes while the discriminator learns to spot the differences between the generator's fakes and the true examples. When the discriminator is able to flag the fake, then the generator is penalized. The feedback loop continues until the generator succeeds in producing output that the discriminator cannot distinguish.
The prime GAN benefit is creating realistic output that can be difficult to distinguish from the originals, which in turn may be used to further train machine learning models. Setting up a GAN to learn is straightforward, since they are trained by using unlabeled data or with minor labeling. However, the potential disadvantage is that the generator and discriminator might go back-and-forth in competition for a long time, creating a large system drain. One training limitation is that a huge amount of input data might be required to obtain a satisfactory output. Another potential problem is “mode collapse,” when the generator produces a limited set of outputs rather than a wider variety.
Diffusion models are generative models that are trained using the forward and reverse diffusion process of progressive noise-addition and denoising. Diffusion models generate data, most often images similar to the data on which they are trained, but then overwrite the data used to train them. They gradually add Gaussian noise to the training data until it’s unrecognizable, then learn a reversed “denoising” process that can synthesize output (usually images) from random noise input.
A diffusion model learns to minimize the differences of the generated samples versus the desired target. Any discrepancy is quantified and the model's parameters are updated to minimize the loss training the model to produce samples closely resembling the authentic training data.
Beyond image quality, diffusion models have the advantage of not requiring adversarial training, which speeds the learning process and also offering close process control. Training is more stable than with GANs and diffusion models are not as prone to mode collapse.
But, compared to GANs, diffusion models can require more computing resources to train, including more fine-tuning. IBM Research® has also discovered that this form of generative AI can be hijacked with hidden backdoors, giving attackers control over the image creation process so that AI diffusion models can be tricked into generating manipulated images.
Transformer models combine an encoder-decoder architecture with a text-processing mechanism and have revolutionized how language models are trained. An encoder converts raw, unannotated text into representations known as embeddings; the decoder takes these embeddings together with previous outputs of the model, and successively predicts each word in a sentence.
Using fill-in-the-blank guessing, the encoder learns how words and sentences relate to each other, building up a powerful representation of language without having to label parts of speech and other grammatical features. Transformers, in fact, can be pretrained at the outset without a particular task in mind. After these powerful representations are learned, the models can later be specialized with much less data to perform a requested task.
Several innovations make this possible. Transformers process words in a sentence simultaneously, enabling text processing in parallel, speeding up training. Earlier techniques including recurrent neural networks (RNNs) processed words one by one. Transformers also learned the positions of words and their relationships, this context enables them to infer meaning and disambiguate words such as “it” in long sentences.
By eliminating the need to define a task upfront, transformers made it practical to pretrain language models on vast amounts of raw text, enabling them to grow dramatically in size. Previously, labeled data was gathered to train one model on a specific task. With transformers, one model trained on a massive amount of data can be adapted to multiple tasks by fine-tuning it on a small amount of labeled task-specific data.
Language transformers today are used for nongenerative tasks such as classification and entity extraction as well as generative tasks including machine translation, summarization and question answering. Transformers have surprised many people with their ability to generate convincing dialog, essays and other content.
Natural language processing (NLP) transformers provide remarkable power since they can run in parallel, processing multiple portions of a sequence simultaneously, which then greatly speeds training. Transformers also track long-term dependencies in text, which enables them to understand the overall context more clearly and create superior output. In addition, transformers are more scalable and flexible in order to be customized by task.
As to limitations, because of their complexity, transformers require huge computational resources and a long training time. Also, the training data must be accurately on-target, unbiased and plentiful to produce accurate results.
Deep learning use cases
The number of uses for deep learning grows every day. Here are just a few of the ways that it is now helping businesses become more efficient and better serve their customers.
Application modernization
Generative AI can enhance the capabilities of developers and reduce the ever-widening skills gap in the domains of application modernization and IT automation. Generative AI for coding is possible because of recent breakthroughs in large language model (LLM) technologies and natural language processing (NLP). It uses deep learning algorithms and large neural networks trained on vast datasets of existing source code. Training code generally comes from publicly available code produced by open-source projects.
Programmers can enter plain text prompts describing what they want the code to do. Generative AI tools suggest code snippets or full functions, streamlining the coding process by handling repetitive tasks and reducing manual coding. Generative AI can also translate code from one language to another, streamlining code conversion or modernization projects, such as updating legacy applications by translating COBOL to Java.
Computer vision is a field of artificial intelligence (AI) that includes image classification, object detection and semantic segmentation. It uses machine learning and neural networks to teach computers and learning systems to derive meaningful information from digital images, videos and other visual inputs and to make recommendations or take actions when the system sees defects or issues. If AI enables computers to think, computer vision enables them to see, observe and understand.
Because a computer vision system is often trained to inspect products or watch production assets, it usually can analyze thousands of products or processes per minute, noticing imperceptible defects or issues. Computer vision is used in industries that range from energy and utilities to manufacturing and automotive.
Computer vision needs lots of data, and then it runs analyses of that data over and over until it discerns and ultimately recognizes images. For example, to train a computer to recognize automobile tires, it needs to be fed vast quantities of tire images and tire-related items to learn the differences and recognize a tire, especially one with no defects.
Computer vision uses algorithmic models to enable a computer to teach itself about the context of visual data. If enough data is fed through the model, the computer will “look” at the data and teach itself to tell one image from another. Algorithms enable the machine to learn by itself, rather than with someone programming it to recognize an image.
Computer vision enables systems to derive meaningful information from digital images, videos and other visual inputs, and based on those inputs, to take action. This ability to provide recommendations distinguishes it from simple image recognition tasks. Some common applications of computer vision today can be seen in:
AI is helping businesses to better understand and cater to increasing consumer demands. With the rise of highly personalized online shopping, direct-to-consumer models, and delivery services, generative AI can help further unlock a host of benefits that can improve customer care, talent transformation and the performance of applications.
AI empowers businesses to adopt a customer-centric approach by harnessing valuable insights from customer feedback and buying habits. This data-driven approach can help improve product design and packaging and can help drive high customer satisfaction and increased sales.
Generative AI can also serve as a cognitive assistant for customer care, providing contextual guidance based on conversation history, sentiment analysis and call center transcripts. Also, generative AI can enable personalized shopping experiences, foster customer loyalty and provide a competitive advantage.
Organizations can augment their workforce by building and deploying robotic process automation (RPA) and digital labor to collaborate with humans to increase productivity, or assist whenever backup is needed. For example, this can help developers speed the updating of legacy software.
Digital labor uses foundation models to automate and improve the productivity of knowledge workers by enabling self-service automation in a fast and reliable way, without technical barriers. To automate task performance or calling APIs, an enterprise-grade LLM-based slot filling model can identify information in a conversation and gather all the information required for completing an action or calling an API without much manual effort.
Instead of having technical experts record and encode repetitive action flows for knowledge workers, digital labor automations built with a foundation of model-powered conversational instructions and demonstrations can be used by the knowledge worker for self-service automation. For example, to speed app creation, no-code digital apprentices can help end-users, who lack programming expertise, by effectively teaching, supervising and validating code.
Generative AI (also called gen AI) is a category of AI that autonomously creates text, images, video, data or other content in response to a user’s prompt or request.
Generative AI relies on deep learning models that can learn from patterns in existing content and generate new, similar content based on that training. It has applications in many fields including customer service, marketing, software development and research offers enormous potential to streamline enterprise workflows through fast, automated content creation and augmentation.
Generative AI excels at handling diverse data sources such as emails, images, videos, audio files and social media content. This unstructured data forms the backbone for creating models and the ongoing training of generative AI, so it can stay effective over time. Using this unstructured data can enhance customer service through chatbots and facilitate more effective email routing. In practice, this might mean guiding users to appropriate resources, whether that’s connecting them with the right agent or directing them to user guides and FAQs.
Despite its much-discussed limitations and risks, many businesses are forging ahead, cautiously exploring how their organizations can harness generative AI to improve their internal workflows, and enhance their products and services. This is the new frontier: How to make the workplace more efficient without creating legal or ethical issues.
AI agents and agentic AI
An AI agent is an autonomous AI program—it can perform tasks and accomplish goals on behalf of a user or another system without human intervention, by designing its own workflow and using available tools (other applications or services).
Agentic AI is a system of multiple AI agents, the efforts of which are coordinated, or orchestrated, to accomplish a more complex task or a greater goal than any single agent in the system could accomplish.
Unlike chatbots and other AI models which operate within predefined constraints and require human intervention, AI agents and agentic AI exhibit autonomy, goal-driven behavior and adaptability to changing circumstances. The terms “agent” and “agentic” refer to these models’ agency, or their capacity to act independently and purposefully.
One way to think of agents is as a natural next step after generative AI. Gen AI models focus on creating content based on learned patterns; agents use that content to interact with each other and other tools to make decisions, solve problems and complete tasks. For example, a gen AI app might be able to tell you the best time to climb Mt. Everest given your work schedule, but an agent can tell you this, and then use an online travel service to book you the best flight and reserve a room in the most convenient hotel in Nepal.
Natural language processing and speech recognition
NLP combines computational linguistics rule-based modeling of human language with statistical and machine learning models to enable computers and digital devices to recognize, understand and generate text and speech. NLP powers applications and devices that can translate text from one language to another, respond to typed or spoken commands, recognize or authenticate users based on voice. It helps summarize large volumes of text, assess the intent or sentiment of text or speech and generate text or graphics or other content on demand.
A subset of NLP is statistical NLP, which combines computer algorithms with machine learning and deep learning models. This approach helps to automatically extract, classify and label elements of text and voice data and then assign a statistical likelihood to each possible meaning of those elements. Today, deep learning models and learning techniques based on RNNs enable NLP systems that “learn” as they work and extract ever more accurate meaning from huge volumes of raw, unstructured and unlabeled text and voice datasets.
Speech recognition: also known as automatic speech recognition (ASR), computer speech recognition or speech-to-text is a capability that enables a program to process human speech into a written format.
While speech recognition is commonly confused with voice recognition, speech recognition focuses on the translation of speech from a verbal format to a text one whereas voice recognition just seeks to identify an individual user’s voice.
Industry applications
Real-world deep learning applications are all around us, and so well integrated into products and services that users are unaware of the complex data processing that is taking place in the background. Some of these examples include:
Customer service deep learning
Many organizations incorporate deep learning technology into their customer service processes. Chatbots are often used in various applications, services and customer service portals. Traditional chatbots use natural language and even visual recognition, commonly found in call center-like menus. However, more sophisticated chatbot solutions attempt to determine, through learning, if there are multiple responses to ambiguous questions in real time. Based on the responses it receives, the chatbot then tries to answer these questions directly or routes the conversation to a human user.
Virtual assistants such as Apple's Siri, Amazon Alexa or Google Assistant extend the idea of a chatbot by enabling speech recognition functionality. This creates a new method to engage users in a personalized way.
Financial services analytics
Financial institutions regularly use predictive analytics to drive algorithmic trading of stocks, assess business risks for loan approvals, detect fraud, and help manage credit and investment portfolios for clients.
Healthcare record-keeping
The healthcare industry has benefited greatly from deep learning capabilities ever since the digitization of hospital records and images. Image recognition applications can support medical imaging specialists and radiologists, helping them analyze and assess more images in less time.
Law enforcement uses deep learning
Deep learning algorithms can analyze and learn from transactional data to identify dangerous patterns that indicate possible fraudulent or criminal activity. Speech recognition, computer vision and other deep learning applications can improve the efficiency and effectiveness of investigative analysis by extracting patterns and evidence from sound and video recordings, images and documents. This capability helps law enforcement analyze large amounts of data more quickly and accurately.
Learn why IBM has been recognized as a Leader in the 2025 Gartner® Magic Quadrant™ for Data Science and Machine Learning Platforms.
Learn why IBM has been recognized as a Leader in the 2025 Gartner® Magic Quadrant™ for Data Science and Machine Learning Platforms.
Activate these five mindshifts to cut through the uncertainty, spur business reinvention, and supercharge growth with agentic AI.
Activate these five mindshifts to cut through the uncertainty, spur business reinvention, and supercharge growth with agentic AI.
Learn fundamental concepts and build your skills with hands-on labs, courses, guided projects, trials and more.
Learn fundamental concepts and build your skills with hands-on labs, courses, guided projects, trials and more.
Learn how to confidently incorporate generative AI and machine learning into your business.
Learn how to confidently incorporate generative AI and machine learning into your business.
Want to get a better return on your AI investments? Learn how scaling gen AI in key areas drives change by helping your best minds build and deliver innovative new solutions.
Want to get a better return on your AI investments? Learn how scaling gen AI in key areas drives change by helping your best minds build and deliver innovative new solutions.
Learn how to select the most suitable AI foundation model for your use case.
Learn how to select the most suitable AI foundation model for your use case.
IBM® Granite™ is our family of open, performant and trusted AI models, tailored for business and optimized to scale your AI applications. Explore language, code, time series and guardrail options.
IBM® Granite™ is our family of open, performant and trusted AI models, tailored for business and optimized to scale your AI applications. Explore language, code, time series and guardrail options.
Dive into the 3 critical elements of a strong AI strategy: creating a competitive edge, scaling AI across the business and advancing trustworthy AI.
Dive into the 3 critical elements of a strong AI strategy: creating a competitive edge, scaling AI across the business and advancing trustworthy AI.
We surveyed 2,000 organizations about their AI initiatives to discover what’s working, what’s not and how you can get ahead.
We surveyed 2,000 organizations about their AI initiatives to discover what’s working, what’s not and how you can get ahead.
Train, validate, tune and deploy generative AI, foundation models and machine learning capabilities with IBM watsonx.ai, a next-generation enterprise studio for AI builders. Build AI applications in a fraction of the time with a fraction of the data.
Move your applications from prototype to production with the help of our AI development solutions.
Reinvent critical workflows and operations by adding AI to maximize experiences, real-time decision-making and business value.
Get one-stop access to capabilities that span the AI development lifecycle. Produce powerful AI solutions with user-friendly interfaces, workflows and access to industry-standard APIs and SDKs.
Powered by IBM watsonx
Powered by IBM watsonx
- Account informationCurrentWe use your email to validate you are who you say you are, to create your IBMid, and to contact you for account related matters.Business emailYour subscription will be delivered in English. You will find an unsubscribe link in every newsletter. You can manage your subscriptions or unsubscribe  here. Refer to our  IBM Privacy Statement for more information.
- Account informationCurrent
- When the gradient is vanishing and is too small, it continues to become smaller, updating the weight parameters until they become insignificant, that is: zero (0). When that occurs, the algorithm is no longer learning.
- Exploding gradients occur when the gradient is too large, creating an unstable model. In this case, the model weights grow too large, and they will eventually be represented as NaN (not a number). One solution to these issues is to reduce the number of hidden layers within the neural network, eliminating some of the complexity in the RNN models.
- The generator creates something: images, video or audio and then producing an output with a twist. For example, a horse can be transformed into a zebra with some degree of accuracy. The result depends on the input and how well-trained the layers are in the generative model for this use case.
- The discriminator is the adversary, where the generative result (fake image) is compared against the real images in the dataset. The discriminator tries to distinguish between the real and fake images, video or audio.
- Automotive: While the age of driverless cars hasn’t entirely arrived, the underlying technology has started to make its way into automobiles, improving driver and passenger safety through features such as lane line detection.
- Healthcare: Computer vision has been incorporated into radiology technology, enabling doctors to better identify cancerous tumors in healthy anatomy.
- Marketing: Social media platforms provide suggestions on who might be in a photograph that has been posted on a profile, making it easier to tag friends in photo albums.
- Retail: Visual search has been incorporated into some e-commerce platforms, enabling brands to recommend items that would complement an existing wardrobe.

--------------------------------------------------

URL: https://www.ibm.com/think/topics/neural-networks
==================================================
What is a neural network?
What is a neural network?
A neural network is a machine learning program, or model, that makes decisions in a manner similar to the human brain, by using processes that mimic the way biological neurons work together to identify phenomena, weigh options and arrive at conclusions.
Every neural network consists of layers of nodes or artificial neurons, an input layer, one or more hidden layers, and an output layer. Each node connects to others, and has its own associated weight and threshold. If the output of any individual node is above the specified threshold value, that node is activated, sending data to the next layer of the network. Otherwise, no data is passed along to the next layer of the network.
Neural networks rely on training data to learn and improve their accuracy over time. Once they are fine-tuned for accuracy, they are powerful tools in computer science and artificial intelligence, allowing us to classify and cluster data at a high velocity. Tasks in speech recognition or image recognition can take minutes versus hours when compared to the manual identification by human experts. One of the best-known examples of a neural network is Google’s search algorithm.
Neural networks are sometimes called artificial neural networks (ANNs) or simulated neural networks (SNNs). They are a subset of machine learning, and at the heart of deep learning models.
The latest AI trends, brought to you by experts
Get curated insights on the most important—and intriguing—AI news. Subscribe to our weekly Think newsletter. See the IBM Privacy Statement.
Thank you! You are subscribed.
Your subscription will be delivered in English. You will find an unsubscribe link in every newsletter. You can manage your subscriptions or unsubscribe  here. Refer to our  IBM Privacy Statement for more information.
Your subscription will be delivered in English. You will find an unsubscribe link in every newsletter. You can manage your subscriptions or unsubscribe here. Refer to our IBM Privacy Statement for more information.
How do neural networks work?
Think of each individual node as its own linear regression model, composed of input data, weights, a bias (or threshold), and an output. The formula would look something like this:
∑wixi + bias = w1x1 + w2x2 + w3x3 + bias
output = f(x) = 1 if ∑w1x1 + b>= 0; 0 if ∑w1x1 + b < 0
Once an input layer is determined, weights are assigned. These weights help determine the importance of any given variable, with larger ones contributing more significantly to the output compared to other inputs. All inputs are then multiplied by their respective weights and then summed. Afterward, the output is passed through an activation function, which determines the output. If that output exceeds a given threshold, it “fires” (or activates) the node, passing data to the next layer in the network. This results in the output of one node becoming in the input of the next node. This process of passing data from one layer to the next layer defines this neural network as a feedforward network.
Let’s break down what one single node might look like using binary values. We can apply this concept to a more tangible example, like whether you should go surfing (Yes: 1, No: 0). The decision to go or not to go is our predicted outcome, or y-hat. Let’s assume that there are three factors influencing your decision-making:
Then, let’s assume the following, giving us the following inputs:
Now, we need to assign some weights to determine importance. Larger weights signify that particular variables are of greater importance to the decision or outcome.
Finally, we’ll also assume a threshold value of 3, which would translate to a bias value of –3. With all the various inputs, we can start to plug in values into the formula to get the desired output.
Y-hat = (1*5) + (0*2) + (1*4) – 3 = 6
If we use the activation function from the beginning of this section, we can determine that the output of this node would be 1, since 6 is greater than 0. In this instance, you would go surfing; but if we adjust the weights or the threshold, we can achieve different outcomes from the model. When we observe one decision, like in the above example, we can see how a neural network could make increasingly complex decisions depending on the output of previous decisions or layers.
In the example above, we used perceptrons to illustrate some of the mathematics at play here, but neural networks leverage sigmoid neurons, which are distinguished by having values between 0 and 1. Since neural networks behave similarly to decision trees, cascading data from one node to another, having x values between 0 and 1 will reduce the impact of any given change of a single variable on the output of any given node, and subsequently, the output of the neural network.
As we start to think about more practical use cases for neural networks, like image recognition or classification, we’ll leverage supervised learning, or labeled datasets, to train the algorithm. As we train the model, we’ll want to evaluate its accuracy using a cost (or loss) function. This is also commonly referred to as the mean squared error (MSE). In the equation below,
Ultimately, the goal is to minimize our cost function to ensure correctness of fit for any given observation. As the model adjusts its weights and bias, it uses the cost function and reinforcement learning to reach the point of convergence, or the local minimum. The process in which the algorithm adjusts its weights is through gradient descent, allowing the model to determine the direction to take to reduce errors (or minimize the cost function). With each training example, the parameters of the model adjust to gradually converge at the minimum.
See this IBM Developer article for a deeper explanation of the quantitative concepts involved in neural networks.
Most deep neural networks are feedforward, meaning they flow in one direction only, from input to output. However, you can also train your model through backpropagation; that is, move in the opposite direction from output to input. Backpropagation allows us to calculate and attribute the error associated with each neuron, allowing us to adjust and fit the parameters of the model(s) appropriately.
Types of neural networks
Neural networks can be classified into different types, which are used for different purposes. While this isn’t a comprehensive list of types, the below would be representative of the most common types of neural networks that you’ll come across for its common use cases:
The perceptron is the oldest neural network, created by Frank Rosenblatt in 1958.
Feedforward neural networks, or multi-layer perceptrons (MLPs), are what we’ve primarily been focusing on within this article. They are comprised of an input layer, a hidden layer or layers, and an output layer. While these neural networks are also commonly referred to as MLPs, it’s important to note that they are actually comprised of sigmoid neurons, not perceptrons, as most real-world problems are nonlinear. Data usually is fed into these models to train them, and they are the foundation for computer vision, natural language processing, and other neural networks.
Convolutional neural networks (CNNs) are similar to feedforward networks, but they’re usually utilized for image recognition, pattern recognition, and/or computer vision. These networks harness principles from linear algebra, particularly matrix multiplication, to identify patterns within an image.
Recurrent neural networks (RNNs) are identified by their feedback loops. These learning algorithms are primarily leveraged when using time-series data to make predictions about future outcomes, such as stock market predictions or sales forecasting.
Neural networks vs. deep learning
Deep Learning and neural networks tend to be used interchangeably in conversation, which can be confusing. As a result, it’s worth noting that the “deep” in deep learning is just referring to the depth of layers in a neural network. A neural network that consists of more than three layers, which would be inclusive of the inputs and the output, can be considered a deep learning algorithm. A neural network that only has two or three layers is just a basic neural network.
To learn more about the differences between neural networks and other forms of artificial intelligence, like machine learning, please read the blog post “AI vs. Machine Learning vs. Deep Learning vs. Neural Networks: What’s the Difference?”
History of neural networks
The history of neural networks is longer than most people think. While the idea of “a machine that thinks” can be traced to the Ancient Greeks, we’ll focus on the key events that led to the evolution of thinking around neural networks, which has ebbed and flowed in popularity over the years:
1943: Warren S. McCulloch and Walter Pitts published “A logical calculus of the ideas immanent in nervous activity” This research sought to understand how the human brain could produce complex patterns through connected brain cells, or neurons. One of the main ideas that came out of this work was the comparison of neurons with a binary threshold to Boolean logic (i.e., 0/1 or true/false statements).
1958: Frank Rosenblatt is credited with the development of the perceptron, documented in his research, “The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain”. He takes McCulloch and Pitt’s work a step further by introducing weights to the equation. Leveraging an IBM 704, Rosenblatt was able to get a computer to learn how to distinguish cards marked on the left vs. cards marked on the right.
1974: While numerous researchers contributed to the idea of backpropagation, Paul Werbos was the first person in the US to note its application within neural networks within his PhD thesis.
1989: Yann LeCun published a paperillustrating how the use of constraints in backpropagation and its integration into the neural network architecture can be used to train algorithms. This research successfully leveraged a neural network to recognize hand-written zip code digits provided by the U.S. Postal Service.
Learn how to confidently incorporate generative AI and machine learning into your business.
Learn how to confidently incorporate generative AI and machine learning into your business.
Get an in-depth understanding of neural networks, their basic functions and the fundamentals of building one.
Get an in-depth understanding of neural networks, their basic functions and the fundamentals of building one.
Learn why IBM has been recognized as a Leader in the 2025 Gartner® Magic Quadrant™ for Data Science and Machine Learning Platforms.
Learn why IBM has been recognized as a Leader in the 2025 Gartner® Magic Quadrant™ for Data Science and Machine Learning Platforms.
IBM® Granite™ is our family of open, performant and trusted AI models, tailored for business and optimized to scale your AI applications. Explore language, code, time series and guardrail options.
IBM® Granite™ is our family of open, performant and trusted AI models, tailored for business and optimized to scale your AI applications. Explore language, code, time series and guardrail options.
We surveyed 2,000 organizations about their AI initiatives to discover what’s working, what’s not and how you can get ahead.
We surveyed 2,000 organizations about their AI initiatives to discover what’s working, what’s not and how you can get ahead.
Learn how to confidently incorporate generative AI and machine learning into your business.
Learn how to confidently incorporate generative AI and machine learning into your business.
Learn how to select the most suitable AI foundation model for your use case.
Learn how to select the most suitable AI foundation model for your use case.
Learn how CEOs can balance the value generative AI can create against the investment it demands and the risks it introduces.
Learn how CEOs can balance the value generative AI can create against the investment it demands and the risks it introduces.
Want to get a better return on your AI investments? Learn how scaling gen AI in key areas drives change by helping your best minds build and deliver innovative new solutions.
Want to get a better return on your AI investments? Learn how scaling gen AI in key areas drives change by helping your best minds build and deliver innovative new solutions.
Train, validate, tune and deploy generative AI, foundation models and machine learning capabilities with IBM watsonx.ai, a next-generation enterprise studio for AI builders. Build AI applications in a fraction of the time with a fraction of the data.
Put AI to work in your business with IBM’s industry-leading AI expertise and portfolio of solutions at your side.
IBM Consulting AI services help reimagine how businesses work with AI for transformation.
Get one-stop access to capabilities that span the AI development lifecycle. Produce powerful AI solutions with user-friendly interfaces, workflows and access to industry-standard APIs and SDKs.
Powered by IBM watsonx
Powered by IBM watsonx
- Account informationCurrentWe use your email to validate you are who you say you are, to create your IBMid, and to contact you for account related matters.Business emailYour subscription will be delivered in English. You will find an unsubscribe link in every newsletter. You can manage your subscriptions or unsubscribe  here. Refer to our  IBM Privacy Statement for more information.
- Account informationCurrent
- Are the waves good? (Yes: 1, No: 0)
- Is the line-up empty? (Yes: 1, No: 0)
- Has there been a recent shark attack? (Yes: 0, No: 1)
- X1 = 1, since the waves are pumping
- X2 = 0, since the crowds are out
- X3 = 1, since there hasn’t been a recent shark attack
- W1 = 5, since large swells don’t come around often
- W2 = 2, since you’re used to the crowds
- W3 = 4, since you have a fear of sharks
- i represents the index of the sample,
- y-hat is the predicted outcome,
- y is the actual value, and
- m is the number of samples.

--------------------------------------------------

URL: https://www.ibm.com/us-en/privacy
==================================================
IBM Privacy Statement
This privacy statement is effective as of 27 May 2025
At IBM we value your privacy and are committed to protecting and processing your personal information responsibly.
This privacy statement describes how IBM collects, uses, and shares personal information about consumers and other individuals within our clients, business partners, supplier and other organizations with which IBM has or contemplates a business relationship. It applies to IBM Corporation and IBM subsidiaries except where a subsidiary presents its own statement without reference to IBM’s.
Where we provide products, services, or applications as a business-to-business provider to a client, the client is responsible for the collection and use of personal information while using these products, services, or applications. This collection and use is covered by the client’s privacy policy, unless otherwise described. Our agreement with the client may allow us to request and collect information about authorized users of these products, services, or applications for reasons of contract management. In this case, this privacy statement, or a supplementary privacy notice, applies.
We may provide additional data privacy information by using a supplementary privacy notice.
Personal Information We Collect and Use
This section describes the various types of information that we collect and how we use it.
It includes information on Your Account, IBM Websites, IBM Applications, Cloud and Online Services, Marketing, Contractual Relationships, Support Services, Protecting You and IBM, IBM Locations, Recruitment and Former Employees, Conducting our Business Operations, Cookies and Similar Technologies, and Children.
The information that we collect and use may include profile information, interactions on webpages, marketing preferences, information to investigate malicious activities, recordings or transcripts of your conversations with us for support purposes, information to improve our business operations, and more.
You can create an account with IBM by creating an IBMid. An IBMid provides IBM with your name, email address, mailing address, and related information that you may provide. We may require an IBMid for certain services, such as the use of IBM Applications, Cloud and Online Services.
We may also store your details from business contact information that you provide to us, or that we collect from your organization, our Business Partners, or our suppliers.
An IBMid uniquely identifies you when you access our websites, make a request or order, or use a product or service. If you log into our websites with an IBMid we may link the information we collect with your account. An IBMid is also used to give you access to IBM Applications, Cloud and Online Services and allows you to manage your contract and billing history. The email address in your IBMid may be used to contact you in relation to any services to which you subscribe.
Business contact information is typically information that you would find on a business card, such as name and business contact details. We use this information to contact or communicate with you about business matters. If we receive business contact information from a third party, such as an IBM Business Partner or supplier, we will confirm that the information was shared appropriately.
We may also combine your business contact information with other business-relevant information, such as information about your professional education, skills, work experience, or other publicly available information, such as business-related blogs, publications, job roles, and certifications. This information may be used to tailor our interactions with you in any part of IBM’s business, for example in the sales process, to maintain a relationship with you, and for post-contractual relationships.
If your account is created by using an email address containing a domain owned by an organization which you are employed by, contracted to, or volunteer for, your organization can:
If your account was created as, or is converted to, an enterprise ID, your organization manages your account information. It is also the authenticating agency for your ID, meaning your access is controlled by your organization’s login service.
If your IBMid is created through an Enterprise Federation Partner (Identity Provider), basic personal information is collected from the Identity Partner and shared with IBM.
Our websites offer ways to communicate with you about us, our products, and services. The information that we collect on websites is used to provide you with access to the website, to operate the website, to improve your experience, and to personalize the way that information is provided to you. If you visit our websites without logging in with an account, we may still collect information that is connected to your website visit.
For more information on the technologies that we use to collect website information, and setting your preferences, see Cookies and Similar Technologies.
We collect information about your use of our websites, such as:
We use this information to improve and personalize your experience with our websites, provide you with content that you may be interested in, create marketing insights, and to improve our websites, online services, and related technologies.
We also collect the information that your browser or device automatically sends, such as:
We use this information to provide you with access to our webpages, improve the webpage view on your device and browser, adapt to your settings and language, and adapt content for relevancy or any legal requirements for your country. We also use this information to comply with system and network security requirements, and to provide support. For more information see, Support Services and Protecting You and IBM.
We also provide platforms and forums that enable online sharing, support, and collaboration among registered members. Any information that you submit to these platforms may be made available to others on the internet, or removed by us, as covered in the platform privacy notice or terms. We are not responsible for any content that you make available through your use of our products or services.
We prepare reports on the use of our websites to derive insights into trending topics and general market knowledge. These reports may be provided to third parties with details on how users interacted with or showed interest in the third-party products or services that were presented on our websites.
We accept no responsibility for the content provided on, or privacy practices, of third-party websites or applications.
IBM Applications, Cloud and Online Services
Our cloud and online services include “as-a-service” and desktop applications, mobile applications (or apps), and IBM Learning services. We collect information about the use of these services, such as pages you view or your interactions on that page, to improve and develop our services and to generate technical and market insights. We may require an IBMid for the use of our cloud and online services (see Your Account).
The information that we collect on our cloud and online services may include:
This information is collected to provide you with access, to operate the service, for support, to personalize and improve your experience of the service, to develop other services and technologies, and generate technical and market insights. For more information on the technologies that we use to collect this information, and setting your preferences, see Cookies and Similar Technologies.
As a public cloud provider, IBM does not use personal information processed under a contract for the purposes of marketing and advertising without express consent.
Mobile application privacy notices may provide details about the information that is collected by the app, such as geo-location information or the unique User-ID of a device. Unique User-IDs are used to connect to servers and to connect the use of the device across apps. Depending on the functions of the app, you can tailor your privacy settings by using the settings menu or in your user profile.
IBM Learning offers education services and collects information on course completions to be able to provide you with credentials, certificates, or further information when needed.
We accept no responsibility for the content provided on, or privacy practices, of third-party websites or applications.
Subject to your preferences, we use the information that we collect to communicate with you about relevant products, services, and offerings. We also use this information to personalize your online experience with our content and advertisements and to develop internal marketing and business intelligence. To set or update your preferences with regards to marketing communications by email, phone, or postal, visit the IBM Privacy Preference Center. You may also submit an opt-out request, or select Unsubscribe at the bottom of each marketing email. To review or set your preferences regarding the information that we collect about you online on our websites, select Cookie Preferences in the website footer.
We use information that we collect for marketing purposes. This may include information:
Subject to your preferences, we may use this information to market to you regarding IBM products, services, and offerings. For example, we may:
To set or update your preferences with regards to marketing activities using your email address, phone, or postal address, visit the IBM Privacy Preference Center.  You may also submit an opt-out request, or select Unsubscribe in each marketing email.
To opt out of the use of your hashed email for personalization or targeted advertising, you can withdraw your email consent by using any of these options.
To review or set your preferences regarding the information that we collect about you online on our websites, select Cookie Preferences in the website footer.
We also use this information to develop marketing and business intelligence, which is essential for our business operations. For example, we may:
Contractual Relationships
A contractual relationship is created when you order a trial, or a product or service from us. While we mainly provide our products and services to businesses, individuals may also enter into an agreement with us directly as a client. We may collect any information that is reasonably necessary to prepare for, enter, and fulfill, the contractual agreement.
The information collected in a contractual relationship may include the business contact information of the requester, an IBMid, and the order details. Information that is required for shipment and payment, for the implementation of services, or to grant access to the product or service may also be collected.
This information may be collected for various purposes, depending on the nature of the products or services, for example, for contractual management and compliance, to provide support, for the improvement or development of our products and services, to contact you for customer satisfaction surveys, and to generate technical and market insights. For more information, see IBM Applications, Cloud and Online Services.
The information collected in a contractual relationship is not used for the purposes of marketing and advertising without obtaining consent before processing.
When you contact us to request support, including through LiveChat, we collect your contact information, problem description, and possible resolutions. We record the information that is provided to handle the support query, for administrative purposes, to foster our relationship with you, for staff training, and for quality assurance purposes.
The information that we collect may include any information exchanged during our phone conversations or provided during Live Chat support sessions on our websites. This may include a recording or transcript of your conversations with us. We may use this information to inform you of products or services that are related to your support request. This can include product updates or fixes, and we may combine the information that is collected through other interactions with you or your organization to provide more valuable suggestions in relation to product support, such as any available training regarding the issue.
While we handle the support case, we may have incidental access to information that you have provided or information that is on your system. This information may contain information about you, your organization's employees, customers, or other relevant parties. The conditions regarding the handling and processing of this information is covered by the applicable Terms of Use or other agreements between your organization and IBM, such as the Terms of Use for Exchanging Diagnostic Data.
Protecting You and IBM
We may collect and use information to protect you and IBM from IT security threats and to secure the information that we hold from unauthorized access, disclosure, alteration, or destruction. This includes information from our IT access authorization systems, such as log-in information.
The security solutions we use to protect your information, our infrastructure, and our networks may collect information such as IP addresses and log files. This is necessary for the functionality and utility of security programs to enable the investigation of any potential security incidents and generate insights on security threats.
We may use specialized tooling and other technical means to collect information at access points to, and in, IT systems and networks to detect unauthorized access, viruses, and indications of malicious activities. The information we collect may be used to conduct investigations when unauthorized access, malware or malicious activities are suspected, and to remove or isolate malicious code or content.
When you visit an IBM location, we collect your name or business contact information (see Your Account), and, in some cases, information from a government issued ID. This information is collected for access management and to protect the security and safety of our locations and employees.
The information that is collected at our locations is used to issue access badges. We may verify the identity of visitors where legally permissible and, for supplier personnel working on site, a badge with a photo identification may be requested for identification purposes.
Camera supervision and access management are used for reasons of security and safety of our locations, employees, and assets. More information may be available at the IBM location.
Recruitment and Former Employees
We are constantly searching for new talent for our organization, and we collect information about job applicants or prospective candidates from several sources. Applicants are referred to the Talent Acquisition Privacy Notice for more information. When an employee leaves IBM, we continue to process information that is related to them for any remaining business, contractual, employment, legal, and fiscal purposes, including the management of pensions to the extent handled by IBM.
Regarding recruitment, we may look for prospective candidates with the help of recruitment intermediaries and may use publicly available information on social media platforms to identify prospective candidates for a specific function.
When an employee leaves IBM, we retain basic information from the former employee about their employment at IBM.
After an employee retires, we process information about the retiree for fulfilling the pension obligations toward the retiree. Information about the processing of pension information, or other retirement programs, can be found with the local organization responsible for pensions. In some countries, this may be an independent organization. In some cases, retirees may still participate in IBM-organized initiatives or programs, such as volunteer and social responsibility programs. Such participation is voluntary, and more information is provided on the relevant websites or information pages for those initiatives.
Conducting our Business Operations
We collect and use information to improve our business operations, systems, and processes. For example, information may be used to conduct, maintain, audit, and optimize our operations, to protect our assets and employees, for product development, and to defend our rights.
We collect information about our business operations to make informed decisions about the organization, the business, and to report on performance, audits, and trends. For example, we use this information to analyze the costs and quality of our operations. Where possible, this is done by using aggregated information, but may use personal information.
We collect and use information from our business systems, which may include personal information, to:
We collect information from the use of our business processes, websites, cloud and online services, products, or technologies. This information may include personal information and is used for product and process development. For example, we may use this information to increase efficiency, decrease costs, or improve services by developing automated processes and tools, or to develop or improve the technologies on which these are based.
Cookies and Similar Technologies
When you visit our websites, cloud and online services, software products, or view our content on certain third-party websites, we collect information regarding your connection and your activity by using various online tracking technologies, such as cookies, web beacons, Local Storage, or HTML5. Information that is collected with these technologies may be necessary to operate the website or service, to improve performance, to help us understand how our online services are used, or to determine the interests of our users. We use advertising partners to provide and assist in the use of such technologies on IBM and other sites.
A cookie is a piece of data that a website may send to your browser, which may be stored on your computer and can be used to identify your computer. Web beacons, including pixels and tags, are technologies that are used to track a user visiting an IBM web page or if a web page was copied to another website. Local Shared Objects can store content information displayed on the webpage visited, and preferences. All of these technologies may be used to provide connected features across our websites or display targeted IBM advertising (subject to your cookie preferences) on other websites based on your interests. Web beacons may also be used to track your interaction with email messages or newsletters, such as to determine whether messages are opened or links are selected.
Session cookies can be used to track your progression from page to page so that you are not asked for information that you have already provided during the current session, or information that is needed to be able to complete a transaction. Session cookies are erased when the web browser is closed.
Persistent cookies issued by IBM expire every 14 months. IBM allows cookies issued by vendors on its websites. For cookies issued by vendors, persistent cookie retention periods are covered by the vendor's privacy policy.
You can use the IBM Cookie Manager to learn more about the online tracking technologies we use and to review or set your preferences regarding the information that we collect about you on our websites. The IBM Cookie Manager is either presented as a notification window when you first visit a webpage or opened by selecting Cookie Preferences in the website footer. The IBM Cookie Manager does not address all types of tracking technologies (for example, email pixels). When using mobile apps, use the options on your mobile device to manage settings.
Blocking, disabling, or rejecting IBM cookies may cause services to not function properly, such as in connection with a shopping cart, or block the use of websites or IBM Cloud services that require you to sign in. Disabling cookies does not disable other online tracking technologies, but prevents the other technologies from accessing any details stored in cookies.
Our websites offer the possibility to use third-party social media options.  If you elect to use these options, these third-party sites may log information about you, such as your IP address, access time, and referring website URLs. If you are logged in to those social media sites, they may also link collected information with your profile information. We accept no responsibility for the privacy practices of these third-party services and encourage you to review their privacy policies for more information.
For information on cookies and how to remove these technologies by using browser settings, see https://www.allaboutcookies.org/.
Unless otherwise indicated, our websites, products, and services are not intended for use by children or minors as specified by law in their jurisdiction.
Sharing Personal Information
We may share your personal information internally and externally with suppliers, advisors, or Business Partners for IBM’s legitimate business purposes, and only on a need-to-know basis. This section describes how we share information and how we facilitate that sharing.
How We Share Personal Information
When sharing personal information, we implement appropriate checks and controls to confirm that the information can be shared in accordance with the applicable law.
If we decide to sell, buy, merge, or otherwise reorganize businesses in some countries, such a transaction may involve disclosing some personal information to prospective or actual business purchasers, or the collection of personal information from those selling such businesses.
Internally, personal information is shared for our business purposes: to improve efficiency, for cost savings, and internal collaboration between our subsidiaries (such as Red Hat). For example, we may share personal information such as managing our relationship with you and other external parties, compliance programs, or systems and networks security.
Our internal access to personal information is restricted and granted only on a need-to-know basis. Sharing of this information is subject to the appropriate intracompany arrangements, our policies, and security standards. For more information, see Legal Basis.
In certain circumstances, personal information may be subject to disclosure to government agencies in accordance with judicial proceedings, court orders, or legal processes. We may also share personal information to protect the rights of IBM or others when IBM believes that such rights may be affected, for example to prevent fraud.
Facilitating International Transfers
Your personal information may be transferred to or accessed by IBM subsidiaries and third parties globally. IBM complies with laws on the transfer of personal information between countries to keep your personal information protected, wherever it may be.
We have implemented various safeguards including:
Controller and Representative Information
IBM does business through its subsidiaries worldwide. The privacy laws in some countries consider a Controller to be the legal entity (or natural person) who defines the purposes for which the processing of personal information takes place and how that information is processed. Parties that are involved in processing operations on behalf of a Controller may be designated as Processors. Designations and associated obligations differ, depending on the jurisdiction.
Where this is relevant for the privacy laws in your country, the Controller of your personal information is IBM’s main subsidiary in your country or region, unless International Business Machines Corporation (IBM Corp.) or another IBM subsidiary identifies itself as the Controller for a specific interaction with you.
IBM Corp. or our Chief Privacy & Responsible Technology Officer can be contacted at:
International Business Machines Corporation,
1, North Castle Drive,
United States of America.
The contact details of the main subsidiary of a country or region can be found here.
Where IBM Corp. or a subsidiary it controls is required to appoint a legal representative, the following representatives have been appointed.
IBM International Group B.V.,
Johan Huizingalaan 765,
IBM United Kingdom Limited,
PO Box 41, North Harbour,
Information Security and Retention
To protect your personal information from unauthorized access, use, and disclosure, we implement reasonable physical, administrative, and technical safeguards. These safeguards include role-based access controls and encryption to keep personal information private while in transit. We also require our Business Partners, suppliers, and third parties to implement appropriate safeguards, such as contract terms and access restrictions, to protect information from unauthorized access, use, and disclosure.
IBM makes artificial intelligence models and systems available, including large language models (often called LLMs). IBM may use such AI models and systems on or embedded in IBM.com properties, including through chatbots. IBM may also make such AI models and systems available under separate terms.
IBM’s AI models and systems are designed, trained, validated, and tested on data from publicly available sources that may incidentally contain Personal Information. We have implemented safeguards, processes, and tools to mitigate associated impacts and help address responsible development and deployment of trustworthy AI.
We only retain personal information as long as necessary to fulfill the purposes for which it is processed, or to comply with legal and regulatory retention requirements. Legal and regulatory retention requirements may include retaining information for:
We retain any contractual relationship information for administrative purposes, legal and regulatory retention requirements, defending IBM rights, and to manage IBM's relationship with you. The information that is provided in a supplementary privacy notice may provide more detailed information on applicable retention terms.
When personal information is no longer needed, we have processes in place to securely delete it, for example by erasing electronic files and shredding physical records.
You may have certain rights when it comes to the handling of your personal information.
The Contact Us form can be used to:
The Submit a data rights request form can be used to:
When you submit a Data Rights (DR) request, you provide us with personal information, including your name and contact details, which we use to respond to your request. In some circumstances, to verify your identity and to ensure we disclose the personal information to the correct individual, we may also request a copy of your photo ID, which is deleted immediately after verification of your identity.
Upon your DR request, your personal information is processed for handling and fulfilling your DR request, in line with IBM's legal obligations and commitments related to DRs.
You can verify the status of requests that you have submitted by using the DR webform in the IBM Trust Center for 90 days after completion of the request. The data relevant to your DR request is retained for a minimum of three years from the date of your last DR-related communication with IBM, to address any request you may have in relation to it, and for IBM's compliance and recording purposes.
For more information about how we process your data, see Personal Information We Collect and Use.
Your rights may be subject to limitations and exceptions resulting from applicable laws. For example, there may be situations where we cannot share certain information that you seek if disclosing this means disclosing information about others.
You may also have the right to complain to the competent supervisory authority. Information about additional rights, when they apply, and the right to complain to the competent supervisory authority can be found here.
If you have an unresolved privacy or data use concern that we have not addressed satisfactorily, please contact our U.S.-based third-party dispute resolution provider (free of charge) here.
To set or update your marketing communications preferences, visit the IBM Privacy Preference Center. You can also submit an opt-out request, or select Unsubscribe at the end of each marketing email.
In some jurisdictions, the lawful handling of personal information is subject to a justification, sometimes referred to as legal basis. The legal bases that we rely on for the lawful handling of your personal information vary depending on the purpose and applicable law.
The different legal bases that we may use are:
We rely on this legal basis when we need to process certain personal information, such as your contact details, payment details, and shipment details, to perform our obligations or to manage our contractual relationship with you.
Legitimate interests relate to being able to conduct and organize business, which includes the marketing of our offerings, protecting our legal interests, securing our IT environment, or meeting client requirements.
We may also process personal information where it is necessary to defend our rights in judicial, administrative, or arbitral proceedings. This also falls under the legal basis of legitimate interest in countries where they are not a separate legal basis.
We process personal information for credit protection, which is a specific legal basis under Brazilian law (LGPD) but is also covered under the legal basis of legitimate interest in other countries.
The processing is based on your consent where we request this.
Where we need to process certain personal information based on our legal obligation. Example:
Privacy Statement Updates
If a material change is made to this Privacy Statement, the effective date is revised, and a notice is posted on the updated Privacy Statement for 30 days. By continuing to use our websites and services after a revision takes effect, it is considered that users have read and understand the changes.
Previous versions of the Privacy Statement are available here.
Sections updated in this release:
We have made our best effort to accurately translate the IBM Privacy Statement into this language. To report any translation concerns, select Contact Us in the header of this page.
- inquire about the status of your account,
- request your account settings (including your personal information), and,
- at its option, convert it to an enterprise ID.
- the webpages you view,
- the amount of time you spend on pages,
- the website URL that referred you to our pages,
- your geographic information derived from your IP address,
- and any hyperlinks you select.
- your browser type and IP address,
- operating system, device type, and version information,
- language settings,
- crash logs,
- IBMid information (if signed in),
- and passwords.
- the pages you view,
- your settings within the service,
- your browser type and IP address,
- operating system, device type, and version information,
- crash logs,
- IBMid information (if signed in),
- and passwords.
- Collected directly from you through your interactions with IBM, such as attendance at events or submission of online registration forms,
- Received from third-party data providers, subject to controls confirming that the third party legally acquired the information and has the right to provide the information to IBM for use in our marketing communications,
- Collected on our websites or from your interactions with IBM emails and content, including content on third-party sites. For more information on the technologies that we use to collect this information, see Cookies and Similar Technologies.
- Contact you by using email, telephone, or postal mail
- Personalize your experience with IBM products and services, such as sharing more relevant content or pre-filling registration forms on our websites.
- Deliver targeted IBM advertisements on third-party websites based on information we or authorized third parties collect about your interactions with IBM websites, our content, emails, or, in select geographies, activity linked to your hashed email address.
- Combine the information we collect to better understand your interests and potential business needs,
- Use aggregated data to measure effectiveness of our marketing campaigns and events, and to proceed to informed business decisions and investments,
- Aggregate the information that is collected about IBM website visitors for the purposes of developing and modelling marketing audiences.
- protect or enforce our rights, including to detect fraud or other criminal activities (for example, by using information in payment systems)
- handle and resolve disputes
- answer complaints and defend IBM in legal proceedings
- and comply with legal obligations in the countries where we do business
- our business with suppliers may include the collection, use, analysis, or other types of processing of personal information on our behalf.
- our business model includes cooperation with independent Business Partners for marketing, selling, and the provision of IBM products and services. Where appropriate (for example, when necessary for the fulfilment of an order), we share business contact information with selected Business Partners.
- we may share personal information with professional advisors, including lawyers, auditors, and insurance companies to receive their services.
- we may share contractual relationship information with others, for instance, our Business Partners, financial institutions, shipping companies, postal, or government authorities, such as the customs authorities that are involved in fulfillment.
- We may share personal information with third parties, such as advertising technology partners, data analytics providers and social networks engaged by IBM to deliver targeted IBM advertisements on their platforms, to aggregate information for analysis, and to track engagement with those advertisements.
- Contractual Clauses, such as those approved by the EU Commission and accepted in several other countries.
- Data Privacy Framework Certification. Where applicable, certain designated IBM services (for example, IBM Infrastructure-as-a-Service, Platform-as-a-Service, Software-as-a-Service, and select other hosted offerings) are certified to comply with the Data Privacy Framework. For more information, see IBM Data Privacy Framework Policy for Certified IBM Cloud Services.
- Binding Corporate Rules for Controllers (IBM BCR-C). We have BCR-C approved by the European Data Protection Authorities and the UK Information Commissioner’s Office. For more information, see IBM Controller Binding Corporate Rules.
- IBM's privacy practices, described in this Privacy Statement, comply with the APEC Cross Border Privacy Rules Framework. The APEC Cross Border Privacy Rules (CBPR) system provides protection of personal information that is transferred among participating APEC economies as it pertains to online information collected through ibm.com.
- audit and accounting purposes,
- statutory retention terms,
- the handling of disputes,
- and the establishment, exercise, or defense of legal claims in the countries where we do business.
- ask questions related to this Privacy Statement and privacy practices. Your message is forwarded to the appropriate member of IBM's Data Privacy Team, including the responsible Data Protection Officers.
- submit a complaint to IBM if you are not satisfied with how IBM is processing your personal information.
- request access to the personal information that we have on you, or have it updated or corrected. Depending on the applicable law, you may have additional rights concerning your personal information.
- request to obtain your personal information in a usable format and transmit it to another party (also known as the right to data portability);
- request to delete the personal information we hold about you;
- Opt-out of specific personal information processing types, such as targeted advertising
- If you intend to purchase a product or service, we require your business contact information to enter into a contract with you or you may need to create an IBMid (see Your Account) to access a purchased product online.
- When fulfilling a contract, you may need to receive support services, for which we will need to collect your contact information.
- We need personal information to consider job applicants or manage the pension entitlements of retirees (see Recruitment and Former Employees).
- We capture your use of, and interaction with our websites to improve them.
- We process your IBMid (see Your Account) to manage access authorization of our services.
- Where we have a contractual relationship with the organization that you are working for, we have a legitimate interest to process your personal information used to manage this contract.
- We process your business contact information (see Your Account) in combination with other business-relevant information to tailor our interactions with you and promote our products and services. We may process your contact information together with details of an IBM event you attended to develop Marketing and business intelligence.
- We process the personal information of applicants based on our legitimate interest to source suitable talent (see Recruitment and Former Employees).
- We have to keep our general business operations functional. To this end we may, for example, processes the login information of our IT systems and networks, or CCTV footage at IBM locations for security and safety purposes.
- the optional use of Cookies and Similar Technologies or email of Marketing materials.
- We may be obliged to ask for a government-issued ID for certain transactions, such as for a financing transaction (see Contractual Relationship).
- Sharing Personal Information
- Controller and Representative Information
- Your Rights
- Your Additional Rights: US States

--------------------------------------------------

